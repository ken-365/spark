{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "spark_guide.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQT8MBvIMicK"
      },
      "source": [
        "## Init setup spark on Google Collab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zJYyBH5_pAK",
        "outputId": "766ad931-30f9-4946-a91b-a7d5edbeb1b1"
      },
      "source": [
        "## connect to Gdrive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "datapath = \"drive/MyDrive/colab/data/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJKDWLXYrCze"
      },
      "source": [
        "# innstall java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXl3vdvx_xME"
      },
      "source": [
        "# sparkSession for spark sql\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "JqSuybm9-kS2",
        "outputId": "2e774961-f47c-4987-c7b9-2e9a47f2a4e0"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e7065982435b:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fcfa59c6e10>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jy5dRaLTfRh"
      },
      "source": [
        "# Part 1 Apache Spark? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugf7bnByTfRi"
      },
      "source": [
        "Single machines do not have enough power and resources to perform\n",
        "computations on huge amounts of information.  \n",
        "A cluster, or group, of computers, pools the resources of\n",
        "many machines together, giving us the ability to use all the cumulative resources as if they were\n",
        "a single computer.   \n",
        "Now, a group of machines alone is not powerful, you need a framework to\n",
        "coordinate work across them. Spark does just that, *managing and coordinating the execution of\n",
        "tasks on data across a cluster of computers*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l06F3nvbTfRi"
      },
      "source": [
        "## Spark architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAWVxvunTfRj"
      },
      "source": [
        "Spark employs a cluster manager that keeps track of the resources available(Spark Applications).\n",
        "\n",
        "**The cluster manager**  \n",
        "responsible for maintaining a cluster of physical machines that will\n",
        "run your Spark Application(s). They are physical machines rather than processes (as they are in Spark)\n",
        "\n",
        "Spark Applications consist of a *driver process* and a set of *executor processes*.\n",
        "\n",
        "**The driver process**  \n",
        "runs your main() function, sits on a node in the cluster, and is responsible for three things:\n",
        "maintaining information about the Spark Application; responding to a user’s program or input;\n",
        "and analyzing, distributing, and scheduling work across the executors\n",
        "\n",
        "**The executors**  \n",
        "responsible for actually carrying out the work that the driver assigns them.\n",
        "Which means executing code assigned to it\n",
        "by the driver, and reporting the state of the computation on that executor back to the driver node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-O-2YgeTfRj"
      },
      "source": [
        "An action begins the process of\n",
        "executing that graph of instructions, as a single job, by breaking it down into stages and tasks to\n",
        "execute across the cluster. The logical structures that we manipulate with transformations and actions\n",
        "are DataFrames and Datasets. To create a new DataFrame or Dataset, you call a transformation. To\n",
        "start computation or convert to native language types, you call an action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67YzNa77TfRj"
      },
      "source": [
        "# Part 4 API overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S8Q4ehgTfRk"
      },
      "source": [
        "## Schemas\n",
        "\n",
        "A schema defines the column names and types of a DataFrame. You can define schemas\n",
        "manually or read a schema from a data source (often called schema on read)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFnfbXbSTfRk"
      },
      "source": [
        "### Value type in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0XodKsZTfRk"
      },
      "source": [
        "| API to access or create a data type                                                                            \t| Value type in Python                                                                                                                                                                                                                                       \t|   \t|\n",
        "|----------------------------------------------------------------------------------------------------------------\t|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\t|---\t|\n",
        "| ByteType()                                                                                                     \t| int or long Note: Numbers will be converted to 1-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -128 to 127.                                                                                                \t|   \t|\n",
        "| ShortType()                                                                                                    \t| int or long Note: Numbers will be converted to 2-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -32768 to 32767.                                                                                            \t|   \t|\n",
        "| IntegerType()                                                                                                  \t| int or long                                                                                                                                                                                                                                                \t|   \t|\n",
        "| LongType()                                                                                                     \t| long Note: Numbers will be converted to 8-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -9223372036854775808 to 9223372036854775807.Otherwise, please convert data to decimal.Decimal and use DecimalType. \t|   \t|\n",
        "| FloatType()                                                                                                    \t| float Note: Numbers will be converted to 4-byte single-precision floating point numbers at runtime.                                                                                                                                                        \t|   \t|\n",
        "| DoubleType()                                                                                                   \t| float                                                                                                                                                                                                                                                      \t|   \t|\n",
        "| DecimalType()                                                                                                  \t| decimal.Decimal                                                                                                                                                                                                                                            \t|   \t|\n",
        "| StringType()                                                                                                   \t| string                                                                                                                                                                                                                                                     \t|   \t|\n",
        "| BinaryType()                                                                                                   \t| bytearray                                                                                                                                                                                                                                                  \t|   \t|\n",
        "| BooleanType()                                                                                                  \t| bool                                                                                                                                                                                                                                                       \t|   \t|\n",
        "| TimestampType()                                                                                                \t| datetime.datetime                                                                                                                                                                                                                                          \t|   \t|\n",
        "| DateType()                                                                                                     \t| datetime.date                                                                                                                                                                                                                                              \t|   \t|\n",
        "| ArrayType(elementType, [containsNull]) Note:The default value of containsNull is True.                         \t| list, tuple, or array                                                                                                                                                                                                                                      \t|   \t|\n",
        "| MapType(keyType, valueType, [valueContainsNull]) Note:The default value of valueContainsNull is True.          \t| dict                                                                                                                                                                                                                                                       \t|   \t|\n",
        "| StructType(fields) Note: fields is a Seq of StructFields. Also, two fields with the same name are not allowed. \t| list or tuple                                                                                                                                                                                                                                              \t|   \t|\n",
        "| StructField(name, dataType, [nullable]) Note: The default value of nullable is True.                           \t| The value type in Python of the data type of this field (For example, Int for a StructField with the data type IntegerType)                                                                                                                                \t|   \t|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgXJ0EpdTfRm"
      },
      "source": [
        "# set data type\n",
        "from pyspark.sql.types import *\n",
        "b = ByteType()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boKratBDTfRm"
      },
      "source": [
        "# Part 5 Overview of Structured API Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FJq2N2mTfRn"
      },
      "source": [
        "Spark takes queries in DataFrames, Datasets, and SQL and compiles them into RDD transformations. It then performs further\n",
        "optimizations at runtime, generating native Java bytecode that can remove entire tasks or stages\n",
        "during execution. Finally the result is returned to the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbf2Bw6XTfRn"
      },
      "source": [
        "## Structured Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGPrOGL-ZI87",
        "outputId": "7bb5a6b3-da29-4828-8868-8b424b32c741"
      },
      "source": [
        "# download sample work data\n",
        "! git clone https://github.com/databricks/Spark-The-Definitive-Guide.git\n",
        "! ls -U Spark-The-Definitive-Guide"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Spark-The-Definitive-Guide' already exists and is not an empty directory.\n",
            "data  code  project-templates  license.md  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwVa_CsIZVC5"
      },
      "source": [
        "datapath = \"Spark-The-Definitive-Guide/data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x490TSMITfRn"
      },
      "source": [
        "# load json to data frame (schema on read, auto detect data types)\n",
        "# df = spark.read.format(\"json\").load(\"data/flight-data/json/2015-summary.json\")\n",
        "df = spark.read.json(datapath+ 'flight-data/json/2015-summary.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV0LBPrGa9sk",
        "outputId": "e1f7b421-a0ef-4cf7-930e-802940156620"
      },
      "source": [
        "%%shell\n",
        "head -3 \"Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"ORIGIN_COUNTRY_NAME\":\"Romania\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":15}\n",
            "{\"ORIGIN_COUNTRY_NAME\":\"Croatia\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":1}\n",
            "{\"ORIGIN_COUNTRY_NAME\":\"Ireland\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":344}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8aZr9mBTfRn",
        "outputId": "9a0e3a03-7d17-44e4-f289-89e61f4eb24b"
      },
      "source": [
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|            Ireland|  344|\n",
            "|            Egypt|      United States|   15|\n",
            "|    United States|              India|   62|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7939eRtTfRo",
        "outputId": "89f2fe18-d151-4d7b-f7b5-b8c9dfc91d6c"
      },
      "source": [
        "# print shape of dataframe : ROW x Column\n",
        "print((df.count(), len(df.columns)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvaZe7ZETfRo"
      },
      "source": [
        "### **View schema**  \n",
        "A schema is a StructType made up of a number of fields, \n",
        "- StructFields, that have a name type \n",
        "- Boolean flag which specifies whether that column can contain missing or null values \n",
        "- Meta data where users store information about this column\n",
        "\n",
        "**Then we can use this output schema to create schema when load**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8GkgzVxTfRo",
        "outputId": "935d6cda-bcfd-4138-e176-73ece2746e6d"
      },
      "source": [
        "df.schema"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXH9OvXlTfRo"
      },
      "source": [
        "# create manually schema (schema on write) programmatically\n",
        "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
        "myManualSchema = StructType([\n",
        "                            StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
        "                            StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
        "                            StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
        "                            ])\n",
        "\n",
        "# df = spark.read.format(\"json\").schema(myManualSchema).load(\"data/flight-data/json/2015-summary.json\")\n",
        "df = spark.read.json(\n",
        "    datapath+ 'flight-data/json/2015-summary.json'\n",
        "    ,schema = myManualSchema\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SOXLfSVTfRo",
        "outputId": "358155e6-a061-45f5-ee45-47f32b955ad7"
      },
      "source": [
        "df.schema"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayGiIzxLc4vj",
        "outputId": "5e989714-cf25-41e2-bd53-19dd79590850"
      },
      "source": [
        "# create schema using DDL langauge\n",
        "DDLSchema = \"\"\"\n",
        "    DEST_COUNTRY_NAME STRING \n",
        "    ,ORIGIN_COUNTRY_NAME STRING\n",
        "    ,count LONG\n",
        "\"\"\"\n",
        "df_dml = spark.read.json(\n",
        "    datapath+ 'flight-data/json/2015-summary.json'\n",
        "    ,schema = DDLSchema\n",
        ")\n",
        "df_dml.schema == df.schema"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEf0GSP5TfRp"
      },
      "source": [
        "### Define and Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImWjqdSvTfRp"
      },
      "source": [
        "construct and refer to columns with two simplest ways by using the $col$ or $column$ functions by pass in column name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c112uR9lTfRp",
        "outputId": "dad63b58-506b-49c0-b9fe-48d7a48deebe"
      },
      "source": [
        "# construct column\n",
        "from pyspark.sql.functions import col, column\n",
        "col('SomeColumnNme') #or\n",
        "column('xyz')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Column<b'xyz'>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36JlGq-ETfRp"
      },
      "source": [
        "**Column as expression**  \n",
        "is a set of transformations on one or more values in a record in a DataFrame.  \n",
        "In the simplest case, expr(\"someCol\") is equivalent to col(\"someCol\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTHXuz7QTfRp",
        "outputId": "41d25241-b94b-4d5d-c53c-eae743ee03b4"
      },
      "source": [
        "(((col(\"someCol\") + 5) * 200) - 6) < col(\"otherCol\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Column<b'((((someCol + 5) * 200) - 6) < otherCol)'>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFiW1VM1TfRq",
        "outputId": "f049b63f-6906-421a-fa8e-3d9d3a4f3794"
      },
      "source": [
        "from pyspark.sql.functions import expr\n",
        "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Column<b'((((someCol + 5) * 200) - 6) < otherCol)'>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R-WuQ25TfRq"
      },
      "source": [
        "**Accessing a DataFrame’s columns**  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBv8ElagTfRq",
        "outputId": "fa0321d9-adcc-4141-a26c-f1066d0a37b4"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
            " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
            " |-- count: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_wZ96liTfRq",
        "outputId": "e7af48c0-8fe7-4ec1-dadb-63c13723ed46"
      },
      "source": [
        "#or get list of the name of each columns\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flc5iNFaTfRr"
      },
      "source": [
        "### Rows and Records\n",
        "\n",
        "Spark represents this record as an object of\n",
        "type Row. Spark manipulates Row objects using column expressions in order to produce usable\n",
        "values. Row objects internally represent arrays of bytes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luhnV9fvTfRr",
        "outputId": "1454a531-933f-409e-c50b-93fa5459741f"
      },
      "source": [
        "# calling first row on our DataFrame:\n",
        "df.first()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXKhWX9CTfRr"
      },
      "source": [
        "#### Create Rows\n",
        "\n",
        "if you create a Row manually, you must specify the values in the same\n",
        "order as the schema of the DataFrame to which they might be appended"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3urpf6joTfRr"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "myRow = Row('Hello',None,1,False) # by position in rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yllxkjxBTfRs"
      },
      "source": [
        "**Accessing Data in rows**\n",
        "\n",
        "Specify the position of row to access"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf5wGl4yTfRs",
        "outputId": "4411bb7a-1949-4257-d4ab-5cfdf6343b0b"
      },
      "source": [
        "myRow[0],myRow[1],myRow[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Hello', None, 1)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMWwwhLiTfRs"
      },
      "source": [
        "## DataFrame Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAxKx79CTfRs"
      },
      "source": [
        "### Creating DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDDEygw9TfRs"
      },
      "source": [
        "# read from json\n",
        "df = spark.read.format('json').load(datapath + '/flight-data/json/2015-summary.json') # schema on the read\n",
        "#register it as a temporary view so that we can query it with SQL\n",
        "df.createOrReplaceTempView(\"dfTable\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sebKtLToTfRs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c483386c-c771-4417-d465-dc109eacdca2"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import Row\n",
        "# or create schema and data frame manually\n",
        "myManualSchema = StructType([ \n",
        "                            StructField(\"some\", StringType(), True),  # column name, type, can contain None\n",
        "                            StructField(\"col\", StringType(), True),\n",
        "                            StructField(\"names\", LongType(), False)\n",
        "                            ])\n",
        "myRow = Row('Hello',None,1)\n",
        "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
        "myDf.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+-----+\n",
            "| some| col|names|\n",
            "+-----+----+-----+\n",
            "|Hello|null|    1|\n",
            "+-----+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ue1jql1TfRt"
      },
      "source": [
        "### select and selectExpr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6AD67dvF-e9",
        "outputId": "e780cd2e-7808-4ea8-b705-928ce32f2ce3"
      },
      "source": [
        "# multiple ways to select column in Spark DataFrame\n",
        "df = spark.range(30).toDF('number') # create spark DataFrame from range\n",
        "\n",
        "# below code have the same result\n",
        "# select col\n",
        "df.select(F.col('number') + 10).show(2)\n",
        "# F.expr\n",
        "df.select(F.expr('number + 10')).show(2)\n",
        "# panda liked\n",
        "df.select(df['number'] + 10).show(2)\n",
        "df.select(df.number + 10).show(2)\n",
        "# selectExpr\n",
        "df.selectExpr('number + 10').show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|(number + 10)|\n",
            "+-------------+\n",
            "|           10|\n",
            "|           11|\n",
            "+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUsATgJzGRtr",
        "outputId": "b350bf9b-91bb-4821-b6c9-d11251698029"
      },
      "source": [
        "df.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|            Ireland|  344|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6XWO6MTTfRt",
        "outputId": "cba20868-08a6-43a0-bbef-fd08afdde3a3"
      },
      "source": [
        "# select(column name) == SQL (SELECT * FROM DEST_COUNTRY_NAME LIMIT 2)\n",
        "df.select('Dest_country_name').show(2) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|Dest_country_name|\n",
            "+-----------------+\n",
            "|    United States|\n",
            "|    United States|\n",
            "+-----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnzBnsoWTfRt",
        "outputId": "ab919a6e-4c7a-4ce8-a724-9498089ff2d8"
      },
      "source": [
        "# select multiple columns\n",
        "df.select('Dest_country_name','count').show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-----+\n",
            "|Dest_country_name|count|\n",
            "+-----------------+-----+\n",
            "|    United States|   15|\n",
            "|    United States|    1|\n",
            "|    United States|  344|\n",
            "+-----------------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSY9J4jfTfRt",
        "outputId": "4f0b302d-4642-4c6d-9a9c-20c02d4df7e2"
      },
      "source": [
        "# Use interchangeably between expr(),col(),column() is equivalent\n",
        "df.select(expr('Dest_country_name'),\n",
        "          col('Dest_country_name'),\n",
        "          column('Dest_country_name')).show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-----------------+-----------------+\n",
            "|Dest_country_name|Dest_country_name|Dest_country_name|\n",
            "+-----------------+-----------------+-----------------+\n",
            "|    United States|    United States|    United States|\n",
            "|    United States|    United States|    United States|\n",
            "+-----------------+-----------------+-----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tKq_LICTfRu",
        "outputId": "cd9bf1d6-06fb-4629-9dd5-6528aeabb687"
      },
      "source": [
        "# expr() is the most flexible (can change column name) using as  == SELECT DEST_COUNTRY_NAME as Destination FROM dfTable LIMIT 2\n",
        "df.select(\n",
        "        expr('Dest_country_name AS Destination')\n",
        "        ).show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+\n",
            "|  Destination|\n",
            "+-------------+\n",
            "|United States|\n",
            "|United States|\n",
            "+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2WRC6F0TfRu",
        "outputId": "c6209b6c-d4c5-4e2a-f782-b5e1f890411b"
      },
      "source": [
        "# or use alias to change name. This will overwrite as\n",
        "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"To\"),\n",
        "         expr('ORIGIN_COUNTRY_NAME').alias('FROM')).show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-------+\n",
            "|           To|   FROM|\n",
            "+-------------+-------+\n",
            "|United States|Romania|\n",
            "|United States|Croatia|\n",
            "+-------------+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1qPCN47TfRu"
      },
      "source": [
        "Simple example that\n",
        "adds a new column withinCountry to our DataFrame that specifies whether the destination and\n",
        "origin are the same:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1zVnC9DTfRu",
        "outputId": "564e98a7-32b0-4d5a-9091-468d5835d91c"
      },
      "source": [
        "# expression to check origin == destination\n",
        "df.select(F.expr('DEST_COUNTRY_NAME == ORIGIN_COUNTRY_NAME').alias('Is travel within same country')).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------+\n",
            "|Is travel within same country|\n",
            "+-----------------------------+\n",
            "|                        false|\n",
            "|                        false|\n",
            "|                        false|\n",
            "+-----------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGh9HNcAGpas",
        "outputId": "7b2c5518-90f5-4a65-f2ba-6c019f50a4be"
      },
      "source": [
        "# using selectExpr\n",
        "df.selectExpr('DEST_COUNTRY_NAME == ORIGIN_COUNTRY_NAME AS Is_same' ).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|Is_same|\n",
            "+-------+\n",
            "|  false|\n",
            "|  false|\n",
            "|  false|\n",
            "+-------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgTgtPQcTfRu",
        "outputId": "a9cae3b7-0889-484c-fc62-64801395851f"
      },
      "source": [
        "# return all(*) + check\n",
        "df.selectExpr(\"*\", # all original columns\n",
        "                \"(DEST_COUNTRY_NAME == ORIGIN_COUNTRY_NAME) as withinCountry\").show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+-------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
            "+-----------------+-------------------+-----+-------------+\n",
            "|    United States|            Romania|   15|        false|\n",
            "|    United States|            Croatia|    1|        false|\n",
            "+-----------------+-------------------+-----+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBCZNlwaG9LF",
        "outputId": "d1b2a14c-c774-4519-9605-7ac206e21384"
      },
      "source": [
        "#using withcolumn\n",
        "df.withColumn('withinCountry', F.col(\"DEST_COUNTRY_NAME\") == F.col(\"ORIGIN_COUNTRY_NAME\")).show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+-------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
            "+-----------------+-------------------+-----+-------------+\n",
            "|    United States|            Romania|   15|        false|\n",
            "|    United States|            Croatia|    1|        false|\n",
            "+-----------------+-------------------+-----+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3c9AbTbTfRv",
        "outputId": "89536e25-e599-4038-c4cc-ab945771a4eb"
      },
      "source": [
        "# aggregation\n",
        "df.selectExpr('count(distinct(DEST_COUNTRY_NAME)) as total_country', 'avg(count) as count_average').show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-------------+\n",
            "|total_country|count_average|\n",
            "+-------------+-------------+\n",
            "|          132|  1770.765625|\n",
            "+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odTrSDLkTfRv"
      },
      "source": [
        "### Converting to Spark Types (Literals)\n",
        "\n",
        "To pass explicit values into Spark that are just a value for all the rows.  \n",
        "This will come up when you might need to check whether a value is greater than some constant\n",
        "or other programmatically created variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9jrvY8uTfRv",
        "outputId": "75671377-e719-455b-8aea-8bb45d6a2986"
      },
      "source": [
        "from pyspark.sql.functions import lit # lit for all rows\n",
        "# use select\n",
        "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(3) # == SELECT *, 1 as One FROM dfTable LIMIT 3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+---+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
            "+-----------------+-------------------+-----+---+\n",
            "|    United States|            Romania|   15|  1|\n",
            "|    United States|            Croatia|    1|  1|\n",
            "|    United States|            Ireland|  344|  1|\n",
            "+-----------------+-------------------+-----+---+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRzjbsvzTfRv"
      },
      "source": [
        "### Adding Columns\n",
        "using the\n",
        "$withColumn$ method on our DataFrame.  \n",
        "Remember that this is just result from query and not able to modify original dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuPcLsprTfRv",
        "outputId": "1ace5c23-c60b-4596-d3eb-87613ea4c999"
      },
      "source": [
        "df.withColumn('One',lit(1)).show(2) # return all column plus new adds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+---+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
            "+-----------------+-------------------+-----+---+\n",
            "|    United States|            Romania|   15|  1|\n",
            "|    United States|            Croatia|    1|  1|\n",
            "+-----------------+-------------------+-----+---+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caA2ppo4TfRw"
      },
      "source": [
        "### Renaming Columns\n",
        "use the $withColumnRenamed$ method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvjeS5KETfRw",
        "outputId": "9459d222-6957-4cdb-9d6c-9465dd7a9faf"
      },
      "source": [
        "df.withColumnRenamed('DEST_COUNTRY_NAME','new name').show(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-------------------+-----+\n",
            "|     new name|ORIGIN_COUNTRY_NAME|count|\n",
            "+-------------+-------------------+-----+\n",
            "|United States|            Romania|   15|\n",
            "+-------------+-------------------+-----+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbxLJ2rpTfRw"
      },
      "source": [
        "### Reserved Characters and Keywords\n",
        "\n",
        "using backtick ( \\` ) when come across any reserved characters like spaces or dashes in column\n",
        "names when referencing a column in an expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8uM35QjTfRw"
      },
      "source": [
        "# new column with space\n",
        "dfWithLongColName = df.withColumn(\n",
        "                                \"This Long Column-Name\", # no need back tick cause not expression\n",
        "                                F.expr(\"ORIGIN_COUNTRY_NAME\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRlQ1UGDTfRw",
        "outputId": "6bfab6c5-3e38-41c6-f810-af87ff69e841"
      },
      "source": [
        "dfWithLongColName.selectExpr(\n",
        "\"`This Long Column-Name`\", # when ref name with space alway put backtick ` in\n",
        "\"`This Long Column-Name` as `new col`\").show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-------+\n",
            "|This Long Column-Name|new col|\n",
            "+---------------------+-------+\n",
            "|              Romania|Romania|\n",
            "|              Croatia|Croatia|\n",
            "+---------------------+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olFWm9LVTfRw"
      },
      "source": [
        "dfWithLongColName.createOrReplaceTempView(\"dfTableLong\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD0386HwTfRx",
        "outputId": "3b82de18-d2c0-4864-abc5-5f27bf253846"
      },
      "source": [
        "# need back tick( `) in expression\n",
        "dfWithLongColName.select(expr(\"`This Long Column-Name`\")).show(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------+\n",
            "|This Long Column-Name|\n",
            "+---------------------+\n",
            "|              Romania|\n",
            "+---------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLP6a7F8TfRx",
        "outputId": "83eebc65-9a3b-4749-eb2f-f2886761116d"
      },
      "source": [
        "# no need when select\n",
        "dfWithLongColName.select('This Long Column-Name').show(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------+\n",
            "|This Long Column-Name|\n",
            "+---------------------+\n",
            "|              Romania|\n",
            "+---------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CYCIzhITfRx"
      },
      "source": [
        "### Removing Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY4QcnyfTfRx",
        "outputId": "8684d2db-681e-4e72-8c57-9447791e3226"
      },
      "source": [
        "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['DEST_COUNTRY_NAME', 'count']"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v21XvgPUTfRx",
        "outputId": "77a68ede-5c07-466a-edb5-720962f61828"
      },
      "source": [
        "# drop multiple columns\n",
        "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['count', 'This Long Column-Name']"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txP19emITfRy"
      },
      "source": [
        "### Changing a Column’s Type (cast)\n",
        "\n",
        "Lets convert our count column from\n",
        "an integer to a type Long:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeKMo4rbH0qE",
        "outputId": "bf92942d-9618-46f0-adb5-cd7e65fe4033"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
            " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
            " |-- count: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0YK0ksQTfRy",
        "outputId": "12385918-e241-4657-939e-4b3efafa3734"
      },
      "source": [
        "df.withColumn(\"count2\", F.col(\"count\").cast(\"string\")).printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
            " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
            " |-- count: long (nullable = true)\n",
            " |-- count2: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETTBtFRVPN7n"
      },
      "source": [
        "### Condition IfElse , CASE WHEN , otherwise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3OEsAXUPNwE",
        "outputId": "d4355627-3d72-4f8e-dcef-6bb68f2bb60e"
      },
      "source": [
        "data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\n",
        "        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\n",
        "        (\"Jen\",\"\",None)]\n",
        "\n",
        "columns = [\"name\",\"gender\",\"salary\"]\n",
        "office = spark.createDataFrame(data = data, schema = columns)\n",
        "office.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+------+\n",
            "|   name|gender|salary|\n",
            "+-------+------+------+\n",
            "|  James|     M| 60000|\n",
            "|Michael|     M| 70000|\n",
            "| Robert|  null|400000|\n",
            "|  Maria|     F|500000|\n",
            "|    Jen|      |  null|\n",
            "+-------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hdp4qy0oPNli",
        "outputId": "3466f1b0-9281-4849-afda-71b0b81038d5"
      },
      "source": [
        "# using sequence when otherwise\n",
        "from pyspark.sql.functions import when\n",
        "office2 = (\n",
        "    office.withColumn('Sex', when(F.col('gender') == 'M', 'Male')\n",
        "                            .when(F.col('gender') == 'F', 'Female') # use expr\n",
        "                            .otherwise('Unspecified'))\n",
        "        # using SQL like expr\n",
        "        .withColumn('Rich', F.expr(\"\"\"CASE  \n",
        "                                WHEN salary >= 100000 THEN 'Y' \n",
        "                                WHEN salary > 100000 THEN 'N'\n",
        "                                ELSE 'Unspecified' END \"\"\")) ## NOTE that you need single quote ('') around string output\n",
        "        # multiple condition & == and , | == or\n",
        "        .withColumn('Man_and_Rich', \n",
        "                    when((F.col('Sex') == 'Male') | (F.col('Rich') == 'Y'), 'Y').otherwise('N') ## NOTE you need () around each condition\n",
        "                    )\n",
        ")\n",
        "office2.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+------+-----------+-----------+------------+\n",
            "|   name|gender|salary|        Sex|       Rich|Man_and_Rich|\n",
            "+-------+------+------+-----------+-----------+------------+\n",
            "|  James|     M| 60000|       Male|Unspecified|           Y|\n",
            "|Michael|     M| 70000|       Male|Unspecified|           Y|\n",
            "| Robert|  null|400000|Unspecified|          Y|           Y|\n",
            "|  Maria|     F|500000|     Female|          Y|           Y|\n",
            "|    Jen|      |  null|Unspecified|Unspecified|           N|\n",
            "+-------+------+------+-----------+-----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnwI1ks0TfRy"
      },
      "source": [
        "### Filtering Rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V3juycXTfRy",
        "outputId": "66522d8f-db0c-4db5-e64a-fe4b19c3a80b"
      },
      "source": [
        "df.where('count < 2').show(2) # ==SELECT * FROM dfTable WHERE count < 2 LIMIT 2\n",
        "df.filter(\"count <2\").show(2) # filter and count use interchangebly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|          Singapore|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 2 rows\n",
            "\n",
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|          Singapore|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDVUnEWwTfRy",
        "outputId": "14a42264-9ee2-47fe-e0f2-c95ad70c26e2"
      },
      "source": [
        "# chain filter\n",
        "df.where('count < 2').where('ORIGIN_COUNTRY_NAME == \"Croatia\"').show()\n",
        "# same thing with below\n",
        "# df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\").show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Croatia|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrkRBuS2IUAv"
      },
      "source": [
        "### Getting Unique Rows (distinct), remove duplicates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r-ybtk5IXJZ",
        "outputId": "c4982eae-dd01-4eba-96fb-4eae84dabea6"
      },
      "source": [
        "df.select(F.col(\"ORIGIN_COUNTRY_NAME\")).distinct().show(3)\n",
        "# same with\n",
        "spark.sql(\"SELECT  DISTINCT(ORIGIN_COUNTRY_NAME) FROM dftable limit 3\").show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|ORIGIN_COUNTRY_NAME|\n",
            "+-------------------+\n",
            "|           Paraguay|\n",
            "|             Russia|\n",
            "|           Anguilla|\n",
            "+-------------------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+-------------------+\n",
            "|ORIGIN_COUNTRY_NAME|\n",
            "+-------------------+\n",
            "|           Paraguay|\n",
            "|             Russia|\n",
            "|           Anguilla|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oKdnIH1L7BZ"
      },
      "source": [
        "# remove combination of duplicated rows\n",
        "df.dropDuplicates(['column1','column2','column3']) # drop only if these 3 column are exactly the same"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDH-vUFiTfRy"
      },
      "source": [
        "### Random Samples and Splits\n",
        "Random splits can be helpful when you need to break up your DataFrame into a random “splits”\n",
        "of the original DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNabxQ4WJAg_",
        "outputId": "fe5b92fc-1bb7-4d1a-d122-c7b99ed03945"
      },
      "source": [
        "seed = 5\n",
        "df.sample(withReplacement= False, fraction=0.4, seed = seed).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|              India|   62|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIh61UtrTfRz",
        "outputId": "ccd2c696-bacb-4739-f6b7-890383b0c366"
      },
      "source": [
        "seed = 99\n",
        "dataFrames = df.randomSplit([0.25, 0.75], seed) # set weight\n",
        "dataFrames[0].count() > dataFrames[1].count() # 2 splits in a list called dataFrames"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgfBp_1RTfRz"
      },
      "source": [
        "### Concatenating and Appending Rows (Union)\n",
        "must be sure that they have the same schema and\n",
        "number of columns; otherwise, the union will fail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5_ZUYTaTfRz",
        "outputId": "2621ef9b-92b3-4866-878b-161dd434c35b"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "schema = df.schema\n",
        "newRows = [Row(\"New Country\", \"Other Country\", 1),\n",
        "            Row(\"New Country 2\", \"Other Country 3\", 1)]\n",
        "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
        "newDF = spark.createDataFrame(parallelizedRows, schema)\n",
        "# Union\n",
        "df.union(newDF)\\\n",
        ".where(\"count = 1\")\\\n",
        ".where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\")\\\n",
        ".show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|          Singapore|    1|\n",
            "|    United States|          Gibraltar|    1|\n",
            "|    United States|             Cyprus|    1|\n",
            "|    United States|            Estonia|    1|\n",
            "|    United States|          Lithuania|    1|\n",
            "|    United States|           Bulgaria|    1|\n",
            "|    United States|            Georgia|    1|\n",
            "|    United States|            Bahrain|    1|\n",
            "|    United States|   Papua New Guinea|    1|\n",
            "|    United States|         Montenegro|    1|\n",
            "|    United States|            Namibia|    1|\n",
            "|      New Country|      Other Country|    1|\n",
            "|    New Country 2|    Other Country 3|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBQWKuVYTfRz"
      },
      "source": [
        "### Sorting Rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVRBxMi7TfR0",
        "outputId": "bf1fdc7c-398d-4cce-ac5a-9917f1c74776"
      },
      "source": [
        "df.sort('count',ascending=False).show(3)\n",
        "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
            "+-----------------+-------------------+------+\n",
            "|    United States|      United States|370002|\n",
            "|    United States|             Canada|  8483|\n",
            "|           Canada|      United States|  8399|\n",
            "+-----------------+-------------------+------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|     Burkina Faso|      United States|    1|\n",
            "|    Cote d'Ivoire|      United States|    1|\n",
            "|           Cyprus|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k9ekhRGTfR0",
        "outputId": "1dcf0b94-17c3-44ef-9f34-5d360e7f3805"
      },
      "source": [
        "from pyspark.sql.functions import desc, asc\n",
        "df.orderBy(expr(\"count desc\"),expr('DEST_COUNTRY_NAME asc')).show(2)\n",
        "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|     Burkina Faso|      United States|    1|\n",
            "|    Cote d'Ivoire|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 2 rows\n",
            "\n",
            "+-----------------+-------------------+------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
            "+-----------------+-------------------+------+\n",
            "|    United States|      United States|370002|\n",
            "|    United States|             Canada|  8483|\n",
            "+-----------------+-------------------+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbziZcqcTfR0"
      },
      "source": [
        "### Repartition and Coalesce\n",
        "Another important optimization opportunity is to partition the data according to some frequently\n",
        "filtered columns, which control the physical layout of data across the cluster including the\n",
        "partitioning scheme and the number of partitions.\n",
        "\n",
        "Repartition will incur a full shuffle of the data, regardless of whether one is necessary. This\n",
        "means that you should typically only repartition when the future number of partitions is greater\n",
        "than your current number of partitions or when you are looking to partition by a set of columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXSHywzgTfR0",
        "outputId": "9cc86e7b-1f4e-4e56-a2af-321335a2c1ce"
      },
      "source": [
        "# get the numbe of current partition\n",
        "df.rdd.getNumPartitions() # 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5AHtMsoTfR0",
        "outputId": "9f0f18a9-0ad7-4314-9ecc-d6d501d7e92d"
      },
      "source": [
        "# partition to five \n",
        "df.repartition(5).rdd.getNumPartitions()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2PCanQyTfR1"
      },
      "source": [
        "If you know that you’re going to be filtering by a certain column often, it can be worth\n",
        "repartitioning based on that column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S1C3psdTfR1",
        "outputId": "3af2d989-5e18-47c3-cb6c-c6a359999321"
      },
      "source": [
        "df.repartition(5, F.col(\"DEST_COUNTRY_NAME\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQQN2iMKTfR1"
      },
      "source": [
        "Coalesce, on the other hand, will not incur a full shuffle and will try to combine partitions. This\n",
        "operation will shuffle your data into five partitions based on the destination country name, and\n",
        "then coalesce them (without a full shuffle):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFjutRlNTfR1",
        "outputId": "ad2bb60a-5e2c-4050-9fa8-180738ead3da"
      },
      "source": [
        "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
            ]
          },
          "execution_count": 193,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWdjP08mTfR1"
      },
      "source": [
        "### Collecting Rows to the Driver (to local machine)\n",
        "There are\n",
        "times when you’ll want to collect some of your data to the driver in order to manipulate it on\n",
        "your **local machine.**\n",
        "\n",
        "$collect()$ gets all data from the entire DataFrame  \n",
        "$take()$ selects the first N rows  \n",
        "$show()$ prints out a number of rows nicely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvtmFbEpTfR2",
        "outputId": "77400c07-3f96-4e82-c216-39dd8a99909e"
      },
      "source": [
        "collectDF = df.limit(10)\n",
        "collectDF.take(5) # take works with an Integer count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
              " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
            ]
          },
          "execution_count": 205,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaM5T4aaTfR2",
        "outputId": "73ce2d44-7825-416d-a636-315aef43a390"
      },
      "source": [
        "collectDF.show(5,  truncate=False)\n",
        "#######\n",
        "collectDF.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|United States    |Romania            |15   |\n",
            "|United States    |Croatia            |1    |\n",
            "|United States    |Ireland            |344  |\n",
            "|Egypt            |United States      |15   |\n",
            "|United States    |India              |62   |\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
              " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n",
              " Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n",
              " Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n",
              " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
            ]
          },
          "execution_count": 209,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lpqrVYkTfR2"
      },
      "source": [
        "# Part 6. Working with Different Types of Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLxsXTaTfR2"
      },
      "source": [
        "Resources fro Spark API doc\n",
        "- DataFrame(Dataset) [Link](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)\n",
        "    - DataFrameStatFunctions :Stat related functions [Link](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions)\n",
        "    - DataFrameNaFunctions :when working with null data [Link](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions)\n",
        "- Column Method: like alias/ contain\n",
        "[Link](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column)\n",
        "- spark.sql.functions  [Link](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ALg9hN-TfR2",
        "outputId": "0b7215af-1168-4c37-bbc4-ffa5b42ed0f5"
      },
      "source": [
        "# read sample dataframe\n",
        "# df = spark.read.format(\"csv\")\\\n",
        "# .option('header','true')\\\n",
        "# .option('inferSchema','true')\\\n",
        "# .load(\"data/retail-data/by-day/2010-12-01.csv\")\n",
        "\n",
        "df = spark.read.csv( \n",
        "    datapath + 'retail-data/by-day/2010-12-01.csv'\n",
        "    ,header=  True\n",
        "    ,inferSchema = True\n",
        ")\n",
        "\n",
        "df.printSchema()\n",
        "df.createOrReplaceTempView(\"dfTable\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- InvoiceNo: string (nullable = true)\n",
            " |-- StockCode: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- InvoiceDate: string (nullable = true)\n",
            " |-- UnitPrice: double (nullable = true)\n",
            " |-- CustomerID: double (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-77dHT7TfR2",
        "outputId": "9df849b5-523f-4e92-a6c7-dd2df58fa638"
      },
      "source": [
        "df.selectExpr('*').show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
            "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
            "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5byyLZuTfR3"
      },
      "source": [
        "## Booleans\n",
        "specify equality as well as\n",
        "less-than or greater-than:\n",
        "Boolean statements consist of four elements: *and*, *or*, *true*, and *false*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6KptV5_TfR3"
      },
      "source": [
        "### AND operation\n",
        "The reason for this is that even if Boolean statements are expressed serially (one after the other),\n",
        "Spark will flatten all of these filters into one statement and perform the filter at the same time,\n",
        "creating the and statement for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4J3_4tRTfR3",
        "outputId": "e2c0527a-216b-4036-cb27-96ff5d850848"
      },
      "source": [
        "# boolean as a filter\n",
        "df.where(col('InvoiceNo') == 536365).where(col('UnitPrice') == 2.55)\\ # Filter\n",
        ".select(col('InvoiceNo').alias('Number_of_Invoice'),col('UnitPrice'),col('Quantity'))\\ # select columns\n",
        ".show(5,False) # False  == no truncate column"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+---------+--------+\n",
            "|Number_of_Invoice|UnitPrice|Quantity|\n",
            "+-----------------+---------+--------+\n",
            "|536365           |2.55     |6       |\n",
            "+-----------------+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLrDTf2VTfR3",
        "outputId": "c757d775-78ba-49ed-a38b-cb7c5b9286fb"
      },
      "source": [
        "# or specify in expression  \n",
        "df.where('InvoiceNo == 536365').where('UnitPrice == 2.55')\\\n",
        ".selectExpr('InvoiceNo as NUmber_of_Invoice','UnitPrice','Quantity')\\\n",
        ".show(5,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+---------+--------+\n",
            "|NUmber_of_Invoice|UnitPrice|Quantity|\n",
            "+-----------------+---------+--------+\n",
            "|536365           |2.55     |6       |\n",
            "+-----------------+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg3NgD68LXxc",
        "outputId": "ccc61fba-d44c-4963-d18e-87578551337f"
      },
      "source": [
        "# You can also using & instead of serialize operation\n",
        "df.where(\n",
        "    (F.col('InvoiceNo') == 536365) & (F.col('UnitPrice') == 2.55)\n",
        ").selectExpr('InvoiceNo as NUmber_of_Invoice','UnitPrice','Quantity')\\\n",
        ".show(5,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---------+--------+\n",
            "|NUmber_of_Invoice|UnitPrice|Quantity|\n",
            "+-----------------+---------+--------+\n",
            "|536365           |2.55     |6       |\n",
            "+-----------------+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "338A9hSDTfR3"
      },
      "source": [
        "### OR operation\n",
        "OR need to have separate variable to store filter rules\n",
        "\n",
        "\n",
        "```\n",
        "use |\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP7f8apfTfR3",
        "outputId": "2eaf4990-e938-40eb-c875-665cde844e5c"
      },
      "source": [
        "from pyspark.sql.functions import instr # == number of position in the string\n",
        "# create filter\n",
        "price_filter = col('UnitPrice') > 600\n",
        "description_filter = instr(df['Description'],'POSTAGE') >= 1 # Postage is not the first word in the description\n",
        "\n",
        "df.where(df.StockCode.isin('DOT'))\\ # have word 'DOT' in StockCode \n",
        ".where(price_filter | description_filter).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
            "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
            "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
            "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7lDJsTHTfR3"
      },
      "source": [
        "To filter a DataFrame, you can also just\n",
        "specify a Boolean column:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29IroYjnTfR5",
        "outputId": "c00cfb30-13aa-47c9-db21-3c1108b2ed95"
      },
      "source": [
        "# Sample query with boolean column in SQL\n",
        "# create temp view for quary in sql\n",
        "df.createOrReplaceTempView('SQL_table1')\n",
        "spark.sql('''\n",
        "SELECT *, (StockCode = 'DOT' AND  \n",
        "    (UnitPrice > 600 OR instr(Description, \"POSTAGE\") >= 1)) as isExpensive  \n",
        "FROM dfTable  \n",
        "WHERE (StockCode = 'DOT' AND (UnitPrice > 600 OR instr(Description, \"POSTAGE\") >= 1))\n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
            "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|isExpensive|\n",
            "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
            "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|       true|\n",
            "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|       true|\n",
            "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLokzlK8TfR5",
        "outputId": "8c1b353c-250e-4b57-c664-fa0b43df0e6f"
      },
      "source": [
        "# can also be done in data frame like\n",
        "# init filter\n",
        "price_filter = col('UnitPrice') > 600\n",
        "description_filter = instr(df['Description'],'POSTAGE') >= 1 \n",
        "dot_filter = df.StockCode.isin('DOT')\n",
        "# query\n",
        "df.where(dot_filter).where(price_filter | description_filter)\\\n",
        ".select(col('*'),(dot_filter & dot_filter | description_filter).alias('isExpensive'))\\\n",
        ".show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
            "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|isExpensive|\n",
            "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
            "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|       true|\n",
            "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|       true|\n",
            "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r8wIpwGTfR5",
        "outputId": "d3db5297-faa5-4511-ffb7-494760a88327"
      },
      "source": [
        "# or use withColumn to add new column\n",
        "df.withColumn(\"isExpensive\", dot_filter  & (price_filter | description_filter))\\ # add true and false base on condition\n",
        ".where(\"isExpensive\")\\ # filter only true\n",
        ".show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
            "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|isExpensive|\n",
            "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
            "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|       true|\n",
            "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|       true|\n",
            "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j97fpci-TfR5"
      },
      "source": [
        "## Numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFfIR82VTfR6"
      },
      "source": [
        "Manipulate number with df functions  \n",
        "For example try $(Quantity * UnitPrice)^2 +5$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap74ANVaTfR6",
        "outputId": "f1dcfeb1-d2f5-4151-ceff-2f61c9508c79"
      },
      "source": [
        "square_quantity = pow(col('Quantity') * col('UnitPrice'),2) +5\n",
        "df.select(expr('CustomerID'),square_quantity.alias('revenue'))\\\n",
        ".show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------------------+\n",
            "|CustomerID|           revenue|\n",
            "+----------+------------------+\n",
            "|   17850.0|239.08999999999997|\n",
            "|   17850.0|          418.7156|\n",
            "|   17850.0|             489.0|\n",
            "+----------+------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymf_1W1_TfR6",
        "outputId": "c0d0d731-07b0-48d9-ac92-1db0d9860245"
      },
      "source": [
        "# in expression\n",
        "df.selectExpr('CustomerID',\n",
        "             'POWER((Quantity * UnitPrice),2.0) +5 as revenue').show(3) # POWER is SQL function that can use in expression"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------------------+\n",
            "|CustomerID|           revenue|\n",
            "+----------+------------------+\n",
            "|   17850.0|239.08999999999997|\n",
            "|   17850.0|          418.7156|\n",
            "|   17850.0|             489.0|\n",
            "+----------+------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKkDA6euTfR6",
        "outputId": "3b541a2a-8896-4d03-ecaf-eb3190e75e6c"
      },
      "source": [
        "# inSQL\n",
        "spark.sql('''\n",
        "SELECT CustomerID, POWER((Quantity * UnitPrice),2.0) +5 as revenue\n",
        "FROM SQL_table1 ''').show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------------------+\n",
            "|CustomerID|           revenue|\n",
            "+----------+------------------+\n",
            "|   17850.0|239.08999999999997|\n",
            "|   17850.0|          418.7156|\n",
            "|   17850.0|             489.0|\n",
            "+----------+------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuYFAvhKTfR6"
      },
      "source": [
        "Rounding up with round and round down with bround"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JqEpEpgTfR7",
        "outputId": "923f9067-d06e-487f-8a7c-889c2f43aa60"
      },
      "source": [
        "from pyspark.sql.functions import lit, round, bround\n",
        "df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+--------------+\n",
            "|round(2.5, 0)|bround(2.5, 0)|\n",
            "+-------------+--------------+\n",
            "|          3.0|           2.0|\n",
            "|          3.0|           2.0|\n",
            "+-------------+--------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEUZZqv2TfR7"
      },
      "source": [
        "compute the correlation of two columns using stat methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD8Qcf85TfR7",
        "outputId": "6be06d66-ad97-4205-a2b1-785d41735c99"
      },
      "source": [
        "# Using stat method\n",
        "df.stat.corr('Quantity','UnitPrice')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.04112314436835551"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqUT7PqaTfR7",
        "outputId": "958b52e8-045c-4425-b3eb-0b5fb55e273a"
      },
      "source": [
        "# in query\n",
        "from pyspark.sql.functions import corr\n",
        "df.select(corr(\"Quantity\", \"UnitPrice\")).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------+\n",
            "|corr(Quantity, UnitPrice)|\n",
            "+-------------------------+\n",
            "|     -0.04112314436835551|\n",
            "+-------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT3SzqGPTfR7"
      },
      "source": [
        "### Compute statistics (using describe())\n",
        "[link text](https://)compute summary statistics for a column or set of columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOKEBcaUTfR7",
        "outputId": "48d2b082-c370-43de-fce0-e077521f889c"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
            "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|        InvoiceDate|         UnitPrice|        CustomerID|       Country|\n",
            "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
            "|  count|             3108|              3108|                3098|              3108|               3108|              3108|              1968|          3108|\n",
            "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128|               null| 4.151946589446603|15661.388719512195|          null|\n",
            "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|               null|15.638659854603892|1854.4496996893627|          null|\n",
            "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|2010-12-01 08:26:00|               0.0|           12431.0|     Australia|\n",
            "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|2010-12-01 17:35:00|            607.49|           18229.0|United Kingdom|\n",
            "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzBFwB-DTfR8"
      },
      "source": [
        "# or compute with these functions\n",
        "from pyspark.sql.functions import count, mean, stddev_pop, min, max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCCLoclHTfR8"
      },
      "source": [
        "calculate either exact or\n",
        "approximate quantiles of your data using the approxQuantile method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PXzMC1NTfR8",
        "outputId": "cc49df53-6eac-49a6-8f0f-5bfaf3841a91"
      },
      "source": [
        "colName = 'UnitPrice'\n",
        "quantileProbs = [0.5]\n",
        "relError = 0.05\n",
        "df.stat.approxQuantile('UnitPrice',quantileProbs,relError)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2.51]"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0vpyFN4TfSA"
      },
      "source": [
        "### Generate ROW number (running number)\n",
        "we can also add a unique ID to each row by using the function\n",
        "monotonically_increasing_id. This function generates a unique value for each row, starting\n",
        "with 0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf_ANNo1TfSA",
        "outputId": "6c56cb8b-baf0-49d7-d397-374990a5b7a5"
      },
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "df.select(col('CustomerID'),monotonically_increasing_id()).show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----------------------------+\n",
            "|CustomerID|monotonically_increasing_id()|\n",
            "+----------+-----------------------------+\n",
            "|   17850.0|                            0|\n",
            "|   17850.0|                            1|\n",
            "+----------+-----------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_0baRLoTfSA"
      },
      "source": [
        "## Strings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r13lSWAJTfSA"
      },
      "source": [
        "initcap sets the first letter of each word in uppercase, all other letters in lowercase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HozK1QpkTfSA",
        "outputId": "1252a362-acd2-4454-91d2-0e21b0b48866"
      },
      "source": [
        "from pyspark.sql.functions import initcap\n",
        "df.select(initcap(col('Country'))).show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+\n",
            "|initcap(Country)|\n",
            "+----------------+\n",
            "|  United Kingdom|\n",
            "|  United Kingdom|\n",
            "+----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92L6RhKpTfSB"
      },
      "source": [
        "Making all strings uppercase or lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7t7HMpOTfSB",
        "outputId": "7c3a5cbb-373a-426d-e0f0-bf487d644bd2"
      },
      "source": [
        "from pyspark.sql.functions import upper, lower\n",
        "df.select(col('Country'),upper(col('Country')),lower(col('Country'))).show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+--------------+--------------+\n",
            "|       Country|upper(Country)|lower(Country)|\n",
            "+--------------+--------------+--------------+\n",
            "|United Kingdom|UNITED KINGDOM|united kingdom|\n",
            "|United Kingdom|UNITED KINGDOM|united kingdom|\n",
            "+--------------+--------------+--------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqwEJ6xcTfSB"
      },
      "source": [
        "adding or removing spaces around a string. You can do this by using lpad,\n",
        "ltrim, rpad and rtrim, trim:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2bjF5Q8TfSB",
        "outputId": "b29b66e3-f9d2-4564-dfdc-5bf1773c4559"
      },
      "source": [
        "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
        "df.select(\n",
        "ltrim(lit(\" HELLO \")).alias(\"ltrim\"),\n",
        "rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n",
        "trim(lit(\" HELLO \")).alias(\"trim\"),\n",
        "lpad(lit(\"HELLO\"), 3, \"*\").alias(\"lp\"), # padding the left side of a string with a specific set of characters.\n",
        "rpad(lit(\"HELLO\"), 10, \"*\").alias(\"rp\")).show(2) # padding the right side of a string with a specific set of characters."
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------+-----+---+----------+\n",
            "| ltrim| rtrim| trim| lp|        rp|\n",
            "+------+------+-----+---+----------+\n",
            "|HELLO | HELLO|HELLO|HEL|HELLO*****|\n",
            "|HELLO | HELLO|HELLO|HEL|HELLO*****|\n",
            "+------+------+-----+---+----------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ01ycVcTfSB"
      },
      "source": [
        "### Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alvFJQZnTfSB"
      },
      "source": [
        "There are two key functions in Spark that you’ll need in\n",
        "order to perform regular expression tasks: regexp_extract and regexp_replace. These\n",
        "functions extract values and replace values, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJwZuw2xTfSC"
      },
      "source": [
        "Let’s explore how to use the regexp_replace function to replace substitute color names in our\n",
        "description column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa7B5sAXN_Gz",
        "outputId": "cddd418e-07ef-4ef9-fb33-0d941b3ef0fe"
      },
      "source": [
        "df.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
            "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
            "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vnxx39vuTfSC",
        "outputId": "144a0e70-fdd9-4d4e-c02d-64f439e6c818"
      },
      "source": [
        "from pyspark.sql.functions import regexp_replace, regexp_extract, col\n",
        "# replace any words in description that match word in strings to COLOR\n",
        "strings= \"(BLACK|WHITE|RED|GREEN|BLUE)\" # replace any of this\n",
        "df.select(regexp_replace(col('Description'),strings,'COLOR').alias('color_clean'), \n",
        "            col('Description')).show(2,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------+----------------------------------+\n",
            "|color_clean                       |Description                       |\n",
            "+----------------------------------+----------------------------------+\n",
            "|COLOR HANGING HEART T-LIGHT HOLDER|WHITE HANGING HEART T-LIGHT HOLDER|\n",
            "|COLOR METAL LANTERN               |WHITE METAL LANTERN               |\n",
            "+----------------------------------+----------------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v87vwWIlTfSC"
      },
      "source": [
        "Another task might be to replace given characters with other characters using translate Building this as a\n",
        "regular expression could be tedious, so Spark also provides the translate function to replace these\n",
        "values.  \n",
        "This is done character by character 'ABCD' , '1234' mean replace A with 1 and replace B with 2 and so on.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdIjM5xOTfSC",
        "outputId": "a23e0b3a-26f4-4b74-e852-252fc9b4ac4a"
      },
      "source": [
        "from pyspark.sql.functions import translate\n",
        "df.select(translate(col('Description'),'ABCD','1234'),'Description').show(2,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------+----------------------------------+\n",
            "|translate(Description, ABCD, 1234)|Description                       |\n",
            "+----------------------------------+----------------------------------+\n",
            "|WHITE H1NGING HE1RT T-LIGHT HOL4ER|WHITE HANGING HEART T-LIGHT HOLDER|\n",
            "|WHITE MET1L L1NTERN               |WHITE METAL LANTERN               |\n",
            "+----------------------------------+----------------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV8nDGzWTfSC"
      },
      "source": [
        "pulling out the first mentioned color using regexp_extract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EzZnyd8TfSC",
        "outputId": "78440efa-afa1-499a-bcf0-2625bad226af"
      },
      "source": [
        "extract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\" # pull out any of this\n",
        "df.select(\n",
        "regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"),\n",
        "col(\"Description\")).show(2,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------------------------+\n",
            "|color_clean|Description                       |\n",
            "+-----------+----------------------------------+\n",
            "|WHITE      |WHITE HANGING HEART T-LIGHT HOLDER|\n",
            "|WHITE      |WHITE METAL LANTERN               |\n",
            "+-----------+----------------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF_BDgHFTfSD"
      },
      "source": [
        "check for their existence. We can do\n",
        "this with the contains method on each column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQFp-PW_TfSD",
        "outputId": "a22e070d-c77e-4bd9-d3b1-c96591b91716"
      },
      "source": [
        "# create column to check has black or white then use boolean result to filter \n",
        "from pyspark.sql.functions import instr\n",
        "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
        "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
        "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n",
        ".where(\"hasSimpleColor\")\\\n",
        ".select(col(\"Description\"),col(\"hasSimpleColor\")).show(3, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------+--------------+\n",
            "|Description                       |hasSimpleColor|\n",
            "+----------------------------------+--------------+\n",
            "|WHITE HANGING HEART T-LIGHT HOLDER|true          |\n",
            "|WHITE METAL LANTERN               |true          |\n",
            "|RED WOOLLY HOTTIE WHITE HEART.    |true          |\n",
            "+----------------------------------+--------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtGrvFrbTfSD"
      },
      "source": [
        "What about 3 or more values  \n",
        "we’re going to use a different function,\n",
        "locate, that returns the integer location (1 based location). We then convert that to a Boolean\n",
        "before using it as the same basic feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ2Lo6EuTfSD",
        "outputId": "6ac623cb-d46c-47ea-c634-2cbecef5babd"
      },
      "source": [
        "from pyspark.sql.functions import  locate\n",
        "simpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n",
        "def color_locator(column, color_string):\n",
        "  return locate(color_string.upper(), column).cast(\"boolean\").alias(\"is_\" + color_string)\n",
        "\n",
        "selectedColumns = [color_locator(df.Description, c) for c in simpleColors]\n",
        "selectedColumns.append(expr(\"*\")) # has to a be Column type\n",
        "\n",
        "df.select(*selectedColumns).where(expr(\"is_white OR is_red\"))\\\n",
        "  .select(\"Description\").show(3, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------+\n",
            "|Description                       |\n",
            "+----------------------------------+\n",
            "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
            "|WHITE METAL LANTERN               |\n",
            "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
            "+----------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4EGfHT9TfSD"
      },
      "source": [
        "## Dates and Timestamps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_nJ4rQ2TfSE"
      },
      "source": [
        "There\n",
        "are dates, which focus exclusively on calendar dates, and timestamps, which include both date\n",
        "and time information.\n",
        "\n",
        "Let’s begin with the basics and get the current date and the current\n",
        "timestamps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIDWblPgTfSE",
        "outputId": "5efa6ebe-fb10-4cbc-9080-392f82c29fa2"
      },
      "source": [
        "from pyspark.sql.functions import current_date,current_timestamp\n",
        "dateDF = spark.range(10)\\\n",
        ".withColumn('today',current_date())\\\n",
        ".withColumn('now',current_timestamp())\n",
        "\n",
        "dateDF.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = false)\n",
            " |-- today: date (nullable = false)\n",
            " |-- now: timestamp (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmjtyBJXTfSE",
        "outputId": "c131af50-3910-43ea-88af-0ac4ab71d5ea"
      },
      "source": [
        "dateDF.show(3,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-----------------------+\n",
            "|id |today     |now                    |\n",
            "+---+----------+-----------------------+\n",
            "|0  |2021-11-13|2021-11-13 21:38:13.754|\n",
            "|1  |2021-11-13|2021-11-13 21:38:13.754|\n",
            "|2  |2021-11-13|2021-11-13 21:38:13.754|\n",
            "+---+----------+-----------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VOxulA8TfSE"
      },
      "source": [
        "let’s add and subtract days from today.\n",
        "These functions take a column and then the number of days to either add or subtract as the\n",
        "arguments:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4icUaljTfSE",
        "outputId": "7a64714d-230e-480e-e473-178774003df6"
      },
      "source": [
        "from pyspark.sql.functions import date_add,date_sub\n",
        "dateDF.select('id','today',date_add(dateDF.today,5), date_sub(dateDF.today,9))\\\n",
        ".show(3,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------------------+------------------+\n",
            "|id |today     |date_add(today, 5)|date_sub(today, 9)|\n",
            "+---+----------+------------------+------------------+\n",
            "|0  |2021-11-13|2021-11-18        |2021-11-04        |\n",
            "|1  |2021-11-13|2021-11-18        |2021-11-04        |\n",
            "|2  |2021-11-13|2021-11-18        |2021-11-04        |\n",
            "+---+----------+------------------+------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rgm0fpLTfSE"
      },
      "source": [
        "Another common task is to take a look at the difference between two dates. We can do this with\n",
        "the datediff function that will return the number of days in between two dates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbFB58dWTfSF",
        "outputId": "a3259f9d-6b2c-4194-bba6-5ce1f481ffeb"
      },
      "source": [
        "from pyspark.sql.functions import datediff\n",
        "dateDF.withColumn('week_ago', date_sub(col('today'),7))\\\n",
        ".select('today','week_ago',datediff(col('today'),col('week_ago'))).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------+-------------------------+\n",
            "|     today|  week_ago|datediff(today, week_ago)|\n",
            "+----------+----------+-------------------------+\n",
            "|2020-05-10|2020-05-03|                        7|\n",
            "|2020-05-10|2020-05-03|                        7|\n",
            "|2020-05-10|2020-05-03|                        7|\n",
            "+----------+----------+-------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LG9h-ivTfSF"
      },
      "source": [
        "The to_date function allows\n",
        "you to convert a string to a date, optionally with a specified forma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-tV5J1TTfSF",
        "outputId": "c01b9728-417f-4861-b936-81829dbe91cf"
      },
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "spark.range(5).withColumn('date',lit('2020-01-01'))\\\n",
        ".select(to_date(col('date')).alias('Cast_string_to_date')).printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Cast_string_to_date: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5m2HV8TTfSF",
        "outputId": "369ca31a-5b18-4cd0-93d0-3e58548f64c1"
      },
      "source": [
        "# if Spark cannot parse the date; it will just return null.\n",
        "# format should be year-month-day -> month 20 is error and return null\n",
        "dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------+---------------------+\n",
            "|to_date('2016-20-12')|to_date('2017-12-11')|\n",
            "+---------------------+---------------------+\n",
            "|                 null|           2017-12-11|\n",
            "+---------------------+---------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-iDviMPTfSF"
      },
      "source": [
        "Let’s fix this pipeline, step by step, and come up with a robust way to avoid these issues entirely.\n",
        "The first step is to remember that we need to specify our date format according to the Java\n",
        "SimpleDateFormat standard. (year - month - date)\n",
        "\n",
        "We will use two functions to fix this: to_date and to_timestamp. The former optionally\n",
        "expects a format, whereas the latter requires one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDT8i4KuTfSG",
        "outputId": "210b863f-19c0-44a8-93a5-c56c3d4c9a74"
      },
      "source": [
        "# if the data comes in year -date- month, we must specify format explicitly\n",
        "dateFormat = 'yyyy-dd-MM'\n",
        "cleanDateDF = spark.range(1).select(\n",
        "                        to_date(lit('2017-12-11'), dateFormat).alias('date1'), # yyyy-dd-MM to yyyy-MM-dd\n",
        "                        to_date(lit('2017-20-05'), dateFormat).alias('date2')\n",
        ").show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------+\n",
            "|     date1|     date2|\n",
            "+----------+----------+\n",
            "|2017-11-12|2017-05-20|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou85b0teTfSG",
        "outputId": "fd6dfca1-14d9-4742-d925-7954fb193877"
      },
      "source": [
        "# time stamp always require format to specify\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "dateDF.select(col('today'),to_timestamp(col('today'),dateFormat)).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----------------------------------+\n",
            "|     today|to_timestamp(`today`, 'yyyy-dd-MM')|\n",
            "+----------+-----------------------------------+\n",
            "|2020-05-10|                2020-05-10 00:00:00|\n",
            "|2020-05-10|                2020-05-10 00:00:00|\n",
            "|2020-05-10|                2020-05-10 00:00:00|\n",
            "+----------+-----------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCvhDspKTfSG"
      },
      "source": [
        "## Working with Nulls in Data\n",
        "There are two things you can do with null values: you can explicitly drop nulls or you can fill\n",
        "them with a value (globally or on a per-column basis)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHULlLYdTfSG"
      },
      "source": [
        "#### Coalesce\n",
        "Returns the value from the first column that is not null. If first column is null then return second and so on\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlN6KXyVTfSG",
        "outputId": "c9d5edfe-af48-4539-eaf4-9d69e49dc4fe"
      },
      "source": [
        "cDf = spark.createDataFrame([(None, None,'James'), (8, 5,'Peak'), (None, 2,'High')], (\"a\", \"b\",'c'))\n",
        "cDf.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-----+\n",
            "|   a|   b|    c|\n",
            "+----+----+-----+\n",
            "|null|null|James|\n",
            "|   8|   5| Peak|\n",
            "|null|   2| High|\n",
            "+----+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSZV1XNtTfSH",
        "outputId": "7d77e809-aa80-4f84-aa5c-ad97db81968b"
      },
      "source": [
        "cDf.select(F.coalesce(cDf[\"a\"], cDf[\"b\"])).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+\n",
            "|coalesce(a, b)|\n",
            "+--------------+\n",
            "|          null|\n",
            "|             8|\n",
            "|             2|\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaDkpoqyTfSH"
      },
      "source": [
        "#### Using SQL functions to manipulate NUll values\n",
        "Naturally, we can use these in select expressions on DataFrames, as well.\n",
        "\n",
        "ifnull \n",
        "- select the second value if the first is null, and defaults to the first.  \n",
        "\n",
        "nullif\n",
        "- returns null if the two values are equal or else returns the second if they are not.\n",
        "\n",
        "nvl \n",
        "- returns the second value if the first is null, but defaults to the first\n",
        "\n",
        "nvl2 \n",
        "- returns the second value if the first is not null; otherwise, it will return the last specified value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4r59sSOTfSH",
        "outputId": "c2f0f874-3c57-4c14-ac3c-676caf4a097f"
      },
      "source": [
        "spark.sql('''\n",
        "SELECT\n",
        "ifnull(null, 'return_value'),\n",
        "nullif('value', 'value'),\n",
        "nvl(null, 'return_value'),\n",
        "nvl2('not_null', 'return_value', \"else_value\")\n",
        "FROM dfTable LIMIT 1\n",
        "''').show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
            "|ifnull(NULL, 'return_value')|nullif('value', 'value')|nvl(NULL, 'return_value')|nvl2('not_null', 'return_value', 'else_value')|\n",
            "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
            "|                return_value|                    null|             return_value|                                  return_value|\n",
            "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCZxMv_NTfSH"
      },
      "source": [
        "#### Drop na\n",
        "The simplest function is drop, which removes rows that contain nulls. The default is to drop any\n",
        "row in which any value is null:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8xBTKclTfSH",
        "outputId": "1a17ada6-de89-412f-d227-e9a51a3fbbb1"
      },
      "source": [
        "# original shape\n",
        "print((df.count(), len(df.columns)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3108, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEp4m_sVTfSI",
        "outputId": "a485de48-d0a4-42bc-bb70-313f2953cb2b"
      },
      "source": [
        "# df.na.drop()\n",
        "print((df.na.drop().count(), len(df.na.drop().columns)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1968, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TLiQgbVTfSI"
      },
      "source": [
        "Using “all” drops the row only if all values are null or NaN for that row:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W9Wfo_UTfSI",
        "outputId": "7ee26e9b-879e-42a9-8343-8d88479a8eda"
      },
      "source": [
        "print((df.na.drop('all').count(), len(df.na.drop('all').columns)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3108, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDzI_7BiTNDh",
        "outputId": "666712e0-1a8c-4074-b8ab-05b2b6189f99"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "na_row  = spark.sql(\"select NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL\")\n",
        "na_row.show()\n",
        "# add null rows\n",
        "print((df.unionAll(na_row).count(), len(df.unionAll(na_row).columns)))\n",
        "# drop only all NULL\n",
        "print((df.unionAll(na_row).na.drop('all').count(), len(df.unionAll(na_row).na.drop('all').columns)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+----+----+----+----+----+\n",
            "|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|\n",
            "+----+----+----+----+----+----+----+----+\n",
            "|null|null|null|null|null|null|null|null|\n",
            "+----+----+----+----+----+----+----+----+\n",
            "\n",
            "(3109, 8)\n",
            "(3108, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPBrzcP-TfSI"
      },
      "source": [
        "#### Fill na\n",
        "For example, to fill all null values in columns of type String, you might specify the following"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLmeVLhnTfSJ",
        "outputId": "bbec74d5-ed8f-46e2-95d8-78e1fe5d3d8d"
      },
      "source": [
        "df.na.fill(\"All Null values become this string\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
            ]
          },
          "execution_count": 226,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R9VdsjHTfSJ"
      },
      "source": [
        "To specify columns, we just pass in an array of column names\n",
        "like we did in the previous example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ8HlWdkTfSJ",
        "outputId": "3ee43cf5-2b24-4707-eca0-90070b4faff3"
      },
      "source": [
        "df.na.fill(0, subset=[\"StockCode\", \"InvoiceNo\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
            ]
          },
          "execution_count": 232,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohaFVr6QTfSJ"
      },
      "source": [
        "#### Replace\n",
        "Value and replacement must be same type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYF76Gr0TfSJ",
        "outputId": "bd9db969-2e39-431e-8883-93fe2bf85d13"
      },
      "source": [
        "cDf.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+-----+\n",
            "|   a|   b|    c|\n",
            "+----+----+-----+\n",
            "|null|null|James|\n",
            "|   8|   5| Peak|\n",
            "|null|   2| High|\n",
            "+----+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc1b5jkJTfSJ",
        "outputId": "d4408830-2daf-4d8a-b4ef-621b4d998aca"
      },
      "source": [
        "# replace Peak with New and High with Maru only in the 'c' column\n",
        "cDf.na.replace(['Peak','High'],['New','Maru'],'c').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+-----+\n",
            "|   a|   b|    c|\n",
            "+----+----+-----+\n",
            "|null|null|James|\n",
            "|   8|   5|  New|\n",
            "|null|   2| Maru|\n",
            "+----+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJCxXy3YTfSK"
      },
      "source": [
        "## Structs, Arrays, Maps(Dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2STqsRUTfSK"
      },
      "source": [
        "#### Structs (DataFrame within DataFrame)\n",
        "We can create a struct by wrapping a set of columns in parenthesis in a query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2o_bpWOTfSK",
        "outputId": "789d86dd-2b10-4c05-8c91-4266c08ad3c2"
      },
      "source": [
        "from pyspark.sql.functions import struct\n",
        "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"), col('Quantity'))\n",
        "complexDF.createOrReplaceTempView(\"complexDF\")\n",
        "complexDF.show(5,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------+--------+\n",
            "|complex                                      |Quantity|\n",
            "+---------------------------------------------+--------+\n",
            "|[WHITE HANGING HEART T-LIGHT HOLDER, 536365] |6       |\n",
            "|[WHITE METAL LANTERN, 536365]                |6       |\n",
            "|[CREAM CUPID HEARTS COAT HANGER, 536365]     |8       |\n",
            "|[KNITTED UNION FLAG HOT WATER BOTTLE, 536365]|6       |\n",
            "|[RED WOOLLY HOTTIE WHITE HEART., 536365]     |6       |\n",
            "+---------------------------------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpvGyFr1TfSK"
      },
      "source": [
        "We can query it just as we might another\n",
        "DataFrame, the only difference is that we use a dot syntax to do so, or the column method\n",
        "getField:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zryp4-KTfSL",
        "outputId": "060fd975-02d6-4ff2-a48b-d491f7c6034f"
      },
      "source": [
        "complexDF.select(\"complex.Description\",'complex.InvoiceNo','Quantity').show(5,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+---------+--------+\n",
            "|Description                        |InvoiceNo|Quantity|\n",
            "+-----------------------------------+---------+--------+\n",
            "|WHITE HANGING HEART T-LIGHT HOLDER |536365   |6       |\n",
            "|WHITE METAL LANTERN                |536365   |6       |\n",
            "|CREAM CUPID HEARTS COAT HANGER     |536365   |8       |\n",
            "|KNITTED UNION FLAG HOT WATER BOTTLE|536365   |6       |\n",
            "|RED WOOLLY HOTTIE WHITE HEART.     |536365   |6       |\n",
            "+-----------------------------------+---------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6wp1gVjTfSL"
      },
      "source": [
        "We can also query all values in the struct by using *. This brings up all the columns to the toplevel\n",
        "DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXImDrL2TfSM",
        "outputId": "c0d68f31-9bbc-4cd4-d60c-85ba83117905"
      },
      "source": [
        "complexDF.select(\"complex.*\",'*').show(3,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------+---------+--------------------------------------------+\n",
            "|Description                       |InvoiceNo|complex                                     |\n",
            "+----------------------------------+---------+--------------------------------------------+\n",
            "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |[WHITE HANGING HEART T-LIGHT HOLDER, 536365]|\n",
            "|WHITE METAL LANTERN               |536365   |[WHITE METAL LANTERN, 536365]               |\n",
            "|CREAM CUPID HEARTS COAT HANGER    |536365   |[CREAM CUPID HEARTS COAT HANGER, 536365]    |\n",
            "+----------------------------------+---------+--------------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0MPBzfzTfSM",
        "outputId": "51aed439-688f-4678-879a-a935cd471633"
      },
      "source": [
        "complexDF.select(\"complex.*\",'*').printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Description: string (nullable = true)\n",
            " |-- InvoiceNo: string (nullable = true)\n",
            " |-- complex: struct (nullable = false)\n",
            " |    |-- Description: string (nullable = true)\n",
            " |    |-- InvoiceNo: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmwkwa8MTfSM"
      },
      "source": [
        "#### Arrays\n",
        "Start by convert column of string in the array of words in column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIE6z2K0TfSM",
        "outputId": "6336ef28-b095-4c6d-c36a-70f109af3a11"
      },
      "source": [
        "from pyspark.sql.functions import split\n",
        "# split description by \" \"\n",
        "tmp = df.select(split(col('Description'),\" \").alias('Des_split'))\n",
        "tmp.show(3,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------------+\n",
            "|Des_split                               |\n",
            "+----------------------------------------+\n",
            "|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|\n",
            "|[WHITE, METAL, LANTERN]                 |\n",
            "|[CREAM, CUPID, HEARTS, COAT, HANGER]    |\n",
            "+----------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oEKx8_TTfSN",
        "outputId": "781bcafa-857b-4189-9c25-bb4ff04f31f4"
      },
      "source": [
        "tmp.selectExpr('Des_split[0]').show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+\n",
            "|Des_split[0]|\n",
            "+------------+\n",
            "|       WHITE|\n",
            "|       WHITE|\n",
            "|       CREAM|\n",
            "+------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj4xqKA_TfSN"
      },
      "source": [
        "##### Array Length\n",
        "determine the array’s length by querying for its size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc5LuoHdTfSN",
        "outputId": "4edbf55d-ce7c-44cd-8a9e-d147d3e574b6"
      },
      "source": [
        "df.select(F.size(split(col('Description'),' ')).alias('Number_of_words(len)')).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|Number_of_words(len)|\n",
            "+--------------------+\n",
            "|                   5|\n",
            "|                   3|\n",
            "|                   5|\n",
            "+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8vzoO7uTfSN"
      },
      "source": [
        "##### array_contains\n",
        "see whether this array contains a value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M99bmxNwTfSN",
        "outputId": "723f2181-8bf3-4195-d321-9fc04b59f96d"
      },
      "source": [
        "df.select(pyf.array_contains(split(col('Description'),' '),'CREAM')).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------+\n",
            "|array_contains(split(Description,  ), CREAM)|\n",
            "+--------------------------------------------+\n",
            "|                                       false|\n",
            "|                                       false|\n",
            "|                                        true|\n",
            "+--------------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3-R3hYuTfSN"
      },
      "source": [
        "##### Explode \n",
        "To convert a complex type into a set of rows\n",
        "(one per value in our array), we need to use the explode function.\n",
        "The explode function takes a column that consists of arrays and creates one row (with the rest of\n",
        "the values duplicated) per value in the array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5soxSi4TfSN",
        "outputId": "e7902ca8-95d2-4761-f512-ab286b9a5bd2"
      },
      "source": [
        "from pyspark.sql.functions import split, explode\n",
        "df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n",
        ".withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
        ".select(\"Description\", \"InvoiceNo\", \"exploded\").show(5,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------+---------+--------+\n",
            "|Description                       |InvoiceNo|exploded|\n",
            "+----------------------------------+---------+--------+\n",
            "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |WHITE   |\n",
            "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HANGING |\n",
            "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HEART   |\n",
            "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |T-LIGHT |\n",
            "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HOLDER  |\n",
            "+----------------------------------+---------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4nTor3KTfSO"
      },
      "source": [
        "##### Map\n",
        "map function and key-value pairs of columns. You then can select\n",
        "them just like you might select from an array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek0KRnKSTfSO",
        "outputId": "53d1d1cf-f955-4cf6-8a64-18625f50f923"
      },
      "source": [
        "from pyspark.sql.functions import create_map\n",
        "df.select(create_map(col('Description'),col('InvoiceNo')).alias('Complex_map')).show(3,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------------------+\n",
            "|Complex_map                                   |\n",
            "+----------------------------------------------+\n",
            "|[WHITE HANGING HEART T-LIGHT HOLDER -> 536365]|\n",
            "|[WHITE METAL LANTERN -> 536365]               |\n",
            "|[CREAM CUPID HEARTS COAT HANGER -> 536365]    |\n",
            "+----------------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXMzVrqITfSO"
      },
      "source": [
        "query them by using the proper key. A missing key returns null:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D68JPk0PTfSO",
        "outputId": "67b42ad4-f6ce-4ec6-b9b1-58642711bd9c"
      },
      "source": [
        "df.select(create_map(col('Description'),col('InvoiceNo')).alias('Complex_map'))\\\n",
        ".selectExpr(\"Complex_map['WHITE METAL LANTERN']\").show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------+\n",
            "|Complex_map[WHITE METAL LANTERN]|\n",
            "+--------------------------------+\n",
            "|                            null|\n",
            "|                          536365|\n",
            "|                            null|\n",
            "+--------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJPUb8RXTfSO"
      },
      "source": [
        "### Working with JSON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEfkljLeTfSO"
      },
      "source": [
        "begin by creating a JSON\n",
        "column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pactTH3PTfSO",
        "outputId": "c8c13eec-acc0-4feb-dfd3-ab1f9659aebf"
      },
      "source": [
        "jsonDF = spark.range(1).selectExpr(\"\"\"\n",
        "                        '{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")\n",
        "jsonDF.show(1,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------------+\n",
            "|jsonString                                 |\n",
            "+-------------------------------------------+\n",
            "|{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}|\n",
            "+-------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7JtdZWvTfSP"
      },
      "source": [
        "Use get_json_object to inline query a JSON object  \n",
        "Use json_tuple if this object has only one level of nesting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfhYra7PTfSP",
        "outputId": "9b21c0f4-e227-427f-d8c4-50fb9a36404b"
      },
      "source": [
        "from pyspark.sql.functions import get_json_object, json_tuple\n",
        "\n",
        "jsonDF.select(\n",
        "    get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias('Column'),\n",
        "    json_tuple(col(\"jsonString\"), \"myJSONKey\")).show(2,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-----------------------+\n",
            "|Column|c0                     |\n",
            "+------+-----------------------+\n",
            "|2     |{\"myJSONValue\":[1,2,3]}|\n",
            "+------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGokUaEzTfSP"
      },
      "source": [
        "turn a StructType into a JSON string by using the to_json function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzkU8KUwTfSP",
        "outputId": "bf164406-033b-4004-f10f-8c8e9e12f72d"
      },
      "source": [
        "from pyspark.sql.functions import to_json\n",
        "new_json = df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
        ".select(to_json(col(\"myStruct\")).alias('newJSON'))\n",
        "\n",
        "new_json.show(3,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------+\n",
            "|newJSON                                                                  |\n",
            "+-------------------------------------------------------------------------+\n",
            "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"}|\n",
            "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}               |\n",
            "|{\"InvoiceNo\":\"536365\",\"Description\":\"CREAM CUPID HEARTS COAT HANGER\"}    |\n",
            "+-------------------------------------------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgHJOK0UTfSP"
      },
      "source": [
        "This function also accepts a dictionary (map) of parameters that are the same as the JSON data\n",
        "source. You can use the from_json function to parse this (or other JSON data) back in. This\n",
        "naturally requires you to specify a schema, and optionally you can specify a map of options, as\n",
        "well:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1egdtp6TfSQ",
        "outputId": "753c9d66-5205-4e27-9dd2-d9efafd75ed3"
      },
      "source": [
        "from pyspark.sql.functions import from_json\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "parseSchema = StructType((\n",
        "    StructField(\"InvoiceNo\",StringType(),True),\n",
        "    StructField(\"Description\",StringType(),True)))\n",
        "\n",
        "new_json.select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(2,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------+-------------------------------------------------------------------------+\n",
            "|jsontostructs(newJSON)                      |newJSON                                                                  |\n",
            "+--------------------------------------------+-------------------------------------------------------------------------+\n",
            "|[536365, WHITE HANGING HEART T-LIGHT HOLDER]|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"}|\n",
            "|[536365, WHITE METAL LANTERN]               |{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}               |\n",
            "+--------------------------------------------+-------------------------------------------------------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u78ZOXCRTfSQ"
      },
      "source": [
        "###  User-Defined Functions UDF\n",
        "UDFs write your own custom\n",
        "transformations using Python.  \n",
        "UDFs can take and return\n",
        "one or more columns as input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehtWkjsnTfSQ"
      },
      "source": [
        "The first step is the actual function. We’ll create a simple one for this example. Let’s write a\n",
        "power3 function that takes a number and raises it to a power of three:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGwdPkcOTfSQ",
        "outputId": "30ddd2f7-0684-41f8-d0dc-f275e84a4bf9"
      },
      "source": [
        "udfExampleDF = spark.range(5).toDF('num')\n",
        "def power3(double_value):\n",
        "    return double_value**3\n",
        "power3(2.0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8.0"
            ]
          },
          "execution_count": 338,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehb8Z8pKTfSQ"
      },
      "source": [
        "Then register the UDFs with Spark to use it on all of our worker machines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI_WSu5zTfSQ"
      },
      "source": [
        "from pyspark.sql.functions import udf\n",
        "power3udf = udf(power3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEAYZkSXTfSQ"
      },
      "source": [
        "Then apply function on column in Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMft6MybTfSR",
        "outputId": "5b8a9b99-1135-4cd9-9cf0-e06449652463"
      },
      "source": [
        "udfExampleDF.select(power3udf(col('num'))).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "|power3(num)|\n",
            "+-----------+\n",
            "|          0|\n",
            "|          1|\n",
            "|          8|\n",
            "+-----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJaJrFpwTfSR"
      },
      "source": [
        "Register function with Spark SQL to be able to use in expression or Spark SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGmZLHs2TfSR",
        "outputId": "85b108fc-afe2-4787-a8cb-4c67a5f7b39e"
      },
      "source": [
        "# register function power3 as 'power3'\n",
        "spark.udf.register('power3',power3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function __main__.power3(double_value)>"
            ]
          },
          "execution_count": 346,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMKqUi6cTfSR",
        "outputId": "f70bb7c9-46b1-4782-b019-f40c1d661b86"
      },
      "source": [
        "# use 'power3' in expression\n",
        "udfExampleDF.selectExpr(\"power3(num)\").show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "|power3(num)|\n",
            "+-----------+\n",
            "|          0|\n",
            "|          1|\n",
            "+-----------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3q8sJLYTfSR",
        "outputId": "23410456-8e04-4327-fe3e-eacff983b450"
      },
      "source": [
        "udfExampleDF.createOrReplaceTempView('udfExampleDF')\n",
        "# use 'power3' in Spark SQL\n",
        "spark.sql(\"\"\"\n",
        "        SELECT *, power3(num) as POWER3\n",
        "        FROM udfExampleDF\n",
        "\"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----------+\n",
            "|num|power3(num)|\n",
            "+---+-----------+\n",
            "|  0|          0|\n",
            "|  1|          1|\n",
            "|  2|          8|\n",
            "|  3|         27|\n",
            "|  4|         64|\n",
            "+---+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg3_dBatTfSR"
      },
      "source": [
        "It is best practice to define the return type\n",
        "of Python function when you define it because Spark type does not align exactly with Python’s types\n",
        "\n",
        "If you specify the type that doesn’t align with the actual type returned by the function, Spark will\n",
        "not throw an error but will just return null to designate a failure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odWekEIzTfSS",
        "outputId": "be07af6d-e64f-49fd-843a-7c86c4597e77"
      },
      "source": [
        "# tell function to return in Double type\n",
        "from pyspark.sql.types import *\n",
        "spark.udf.register('power3py',power3,DoubleType()) # specify double type when register function\n",
        "# use function in expression\n",
        "udfExampleDF.selectExpr(\"power3py(num)\").show()\n",
        "# since the result are integer DoubleType(float) will split out null. This should be Longtype or fixed with cast return in function to float type"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+\n",
            "|power3py(num)|\n",
            "+-------------+\n",
            "|         null|\n",
            "|         null|\n",
            "|         null|\n",
            "|         null|\n",
            "|         null|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPjz9nmvYhwX"
      },
      "source": [
        "### Pandas UDFs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snqOxo8fYj9R",
        "outputId": "7361d47c-d116-428c-e39a-a6b4a5b3313f"
      },
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "@pandas_udf('long')\n",
        "def panda_plus(v):\n",
        "    return v + 1\n",
        "\n",
        "df = spark.range(3)\n",
        "df.withColumn('plus_one', panda_plus('id')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "| id|plus_one|\n",
            "+---+--------+\n",
            "|  0|       1|\n",
            "|  1|       2|\n",
            "|  2|       3|\n",
            "+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w_IPTGRYkcW",
        "outputId": "630a7d88-8281-476e-ddbd-bb517f54ad10"
      },
      "source": [
        "df1 = spark.createDataFrame(\n",
        "    [(1201, 1, 1.0), (1201, 2, 2.0), (1202, 1, 3.0), (1202, 2, 4.0)],\n",
        "    (\"time\", \"id\", \"v1\"))\n",
        "df2 = spark.createDataFrame(\n",
        "    [(1201, 1, \"x\"), (1201, 2, \"y\")], (\"time\", \"id\", \"v2\"))\n",
        "\n",
        "def asof_join(left: pd.DataFrame, right: pd.DataFrame) -> pd.DataFrame:\n",
        "    return pd.merge_asof(left, right, on=\"time\", by=\"id\")\n",
        "\n",
        "df1.groupby(\"id\").cogroup(\n",
        "    df2.groupby(\"id\")\n",
        ").applyInPandas(asof_join, \"time int, id int, v1 double, v2 string\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+---+---+\n",
            "|time| id| v1| v2|\n",
            "+----+---+---+---+\n",
            "|1201|  1|1.0|  x|\n",
            "|1202|  1|3.0|  x|\n",
            "|1201|  2|2.0|  y|\n",
            "|1202|  2|4.0|  y|\n",
            "+----+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayIhiEuDTfSS"
      },
      "source": [
        "# Part 7 Aggregations\n",
        "\n",
        "| Aggregation  | Description                                                                                                                                                                           |\n",
        "|--------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| group by     | specify one or more keys as well as one or more aggregation functions to transform the value columns                                                                                  |\n",
        "| window       | specify one or more keys as well as one or more aggregation functions to transform the value columns. However, the rows input to the function are somehow related to the current row. |\n",
        "| grouping set | aggregate at multiple different levels. Grouping sets are available as a primitive in SQL and via rollups and cubes in DataFrames                                                     |\n",
        "| rollup       | specify one or more keys as well as one or more aggregation functions to transform the value columns, which will be summarized hierarchically.                                        |\n",
        "| cube         | specify one or more keys as well as one or more aggregation functions to transform the value columns, which will be summarized across all combinations of columns.                    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M5PyznmTfSS"
      },
      "source": [
        "Let’s read, repartitioning the data to have far fewer\n",
        "partitions (because we know it’s a small volume of data stored in a lot of small files), and\n",
        "caching the results for rapid access"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6dFDnljTfSS"
      },
      "source": [
        "datapath = 'Spark-The-Definitive-Guide/data/'\n",
        "df = spark.read.format('csv')\\\n",
        ".option('header','true')\\\n",
        ".option('inferShema','true')\\\n",
        ".load(datapath +\"retail-data/all/*.csv\")\\\n",
        ".coalesce(5)\n",
        "#This will put a copy of the intermediately transformed\n",
        "#dataset into memory, allowing us to repeatedly access it at much lower cost than running the\n",
        "# entire pipeline again.\n",
        "df.cache() \n",
        "df.createOrReplaceTempView(\"dfTable\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER_eJb9MTfSS"
      },
      "source": [
        "### Aggregation Functions\n",
        "Most functions are in pyspark.sql.function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4ABcaw6TfSS"
      },
      "source": [
        "#### Count\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NC4KDWlTfSS",
        "outputId": "221abcb0-f014-459f-e161-485f5e307c6e"
      },
      "source": [
        "# Count as transformation\n",
        "from pyspark.sql.functions import count\n",
        "df.select(count(\"*\")).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|  541909|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vxPjYa5TfST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323f9960-ea56-4360-95dc-14ff6bdf041f"
      },
      "source": [
        "# Count as action (return immediately) \n",
        "df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "541909"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc560XWITfST"
      },
      "source": [
        "Count distinct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRu7ipaKTfST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a477021-8058-40e0-aea9-54356f5223e1"
      },
      "source": [
        "from pyspark.sql.functions import countDistinct\n",
        "df.select(countDistinct(\"StockCode\")).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+\n",
            "|count(DISTINCT StockCode)|\n",
            "+-------------------------+\n",
            "|                     4070|\n",
            "+-------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu-VqPC1TfST"
      },
      "source": [
        "When work with large datasets and the exact distinct count is irrelevant.  \n",
        "Use the approx_count_distinct function for an approximation to a certain degree of accuracy(less accuracy, faster performance)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKt7KbMiTfST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f716919-df8b-447b-91b1-2608fa16a9dd"
      },
      "source": [
        "from pyspark.sql.functions import approx_count_distinct\n",
        "df.select(approx_count_distinct('StockCode',rsd=0.04)).show() # less rsd, more accurate, more time to process"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------+\n",
            "|approx_count_distinct(StockCode)|\n",
            "+--------------------------------+\n",
            "|                            4122|\n",
            "+--------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSdRiQUvTfST"
      },
      "source": [
        "#### First and last\n",
        "Get first and last value from DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELrrApWYTfST",
        "outputId": "4fc633fd-5b1e-45e2-bd57-67457b2a2682"
      },
      "source": [
        "from pyspark.sql.functions import first,last\n",
        "df.select(first('StockCode'),last('StockCode')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------+----------------------+\n",
            "|first(StockCode, false)|last(StockCode, false)|\n",
            "+-----------------------+----------------------+\n",
            "|                 85123A|                 22138|\n",
            "+-----------------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5s9zzlETfST"
      },
      "source": [
        "#### Min and Max"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YntohqATfSU",
        "outputId": "43f35629-290b-4b9e-e541-765becc46d7e"
      },
      "source": [
        "# from pyspark.sql.functions import min,max\n",
        "import pyspark.sql.functions as pyf\n",
        "\n",
        "df.select(pyf.min('Quantity'),pyf.max('Quantity')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-------------+\n",
            "|min(Quantity)|max(Quantity)|\n",
            "+-------------+-------------+\n",
            "|           -1|          992|\n",
            "+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvUhTKEaTfSU"
      },
      "source": [
        "#### Sum, sumDistinct\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbrtEeBbTfSU",
        "outputId": "ba5566ec-a402-42a6-d937-b3b97125e3bc"
      },
      "source": [
        "df.select(pyf.sum('Quantity'), pyf.sumDistinct('Quantity')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+----------------------+\n",
            "|sum(Quantity)|sum(DISTINCT Quantity)|\n",
            "+-------------+----------------------+\n",
            "|    5176450.0|               29310.0|\n",
            "+-------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPvyEud_TfSU"
      },
      "source": [
        "#### avg(Average)\n",
        "Ways to compute average   \n",
        "1. sum(*)/count(*) \n",
        "2. avg(*)\n",
        "3. expr(mean(*)) #using sql functions 'mean' in expresssion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YcK_dcQTfSU",
        "outputId": "ef3ea63e-b45d-4fb7-dcf1-2fa4357696bc"
      },
      "source": [
        "df.select(\n",
        "    pyf.count(\"Quantity\").alias(\"total_transactions\"),\n",
        "    pyf.sum(\"Quantity\").alias(\"total_purchases\"),\n",
        "    pyf.avg(\"Quantity\").alias(\"avg_purchases\"),\n",
        "    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
        ".selectExpr(\n",
        "    \"total_purchases/total_transactions\",\n",
        "    \"avg_purchases\",\n",
        "    \"mean_purchases\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------+----------------+----------------+\n",
            "|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n",
            "+--------------------------------------+----------------+----------------+\n",
            "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
            "+--------------------------------------+----------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBZqbQ-FTfSU"
      },
      "source": [
        "#### Variance and Standard Deviation\n",
        "By default, Spark performs the formula for\n",
        "the sample standard deviation or variance if you use the $variance$ or $stddev$ functions.  \n",
        "You can also specify these explicitly or refer to the population standard deviation or variance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ao-vRGMTfSU",
        "outputId": "953c0c7a-e885-49fe-f98e-63b526c25e6a"
      },
      "source": [
        "df.select(pyf.var_pop(\"Quantity\"), pyf.var_samp(\"Quantity\"), # sample statistic\n",
        "pyf.stddev_pop(\"Quantity\"), pyf.stddev_samp(\"Quantity\")).show()  # population statistic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+------------------+--------------------+---------------------+\n",
            "|var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
            "+-----------------+------------------+--------------------+---------------------+\n",
            "|47559.30364660879| 47559.39140929848|  218.08095663447733|   218.08115785023355|\n",
            "+-----------------+------------------+--------------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9nT75wATfSU"
      },
      "source": [
        "#### Skewness and kurtosis\n",
        "both measurements of extreme points in your data.  \n",
        "Skewness \n",
        "measures the asymmetry of the values in your data around the mean, whereas kurtosis is a\n",
        "measure of the tail of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8pWYWEYTfSU",
        "outputId": "8fc9cf74-1964-4413-9ae5-541255702a18"
      },
      "source": [
        "from pyspark.sql.functions import skewness, kurtosis\n",
        "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+------------------+\n",
            "|skewness(Quantity)|kurtosis(Quantity)|\n",
            "+------------------+------------------+\n",
            "|-0.264075576105298|119768.05495534067|\n",
            "+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCtcHUU2TfSV"
      },
      "source": [
        "#### Covariance and Correlation\n",
        "\n",
        "Correlation(corr) measures the Pearson correlation\n",
        "coefficient, which is scaled between –1 and +1.   \n",
        "The covariance(cov) is scaled according to the inputs\n",
        "in the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ5mLZOWTfSV",
        "outputId": "ab6c8914-ef1f-40c1-f9d9-20d4dac0d2b9"
      },
      "source": [
        "from pyspark.sql.functions import corr, covar_pop, covar_samp,\n",
        "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"), # sample\n",
        "covar_pop(\"InvoiceNo\", \"Quantity\")).show() # population"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------+-------------------------------+------------------------------+\n",
            "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
            "+-------------------------+-------------------------------+------------------------------+\n",
            "|     4.912186085636837E-4|             1052.7280543912716|            1052.7260778751674|\n",
            "+-------------------------+-------------------------------+------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXv9BegNTfSV"
      },
      "source": [
        "#### Aggregating to Complex Types\n",
        "collect_list = return all values in this column in single list  \n",
        "collect_set = return all **distinct** values in a set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNluC48jTfSV",
        "outputId": "c212ac60-a378-48d3-d986-ba83f0cffe2b"
      },
      "source": [
        "from pyspark.sql.functions import collect_set, collect_list\n",
        "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------------------+\n",
            "|collect_set(Country)|collect_list(Country)|\n",
            "+--------------------+---------------------+\n",
            "|[Portugal, Italy,...| [United Kingdom, ...|\n",
            "+--------------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f7m578FTfSV"
      },
      "source": [
        "## Grouping\n",
        "calculations based on *groups*(categorical) in the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DMrxWOsTfSV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48feaef6-d883-41a1-e008-04f202c408b7"
      },
      "source": [
        "#sample\n",
        "df.groupBy('InvoiceNo','CustomerId').count().show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+-----+\n",
            "|InvoiceNo|CustomerId|count|\n",
            "+---------+----------+-----+\n",
            "|   536395|     13767|   14|\n",
            "|   536609|     17850|   16|\n",
            "|   536785|     15061|    6|\n",
            "+---------+----------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzVMe2oyTfSW"
      },
      "source": [
        "### Grouping with Expressions\n",
        "\n",
        "specify it as within agg makes it possible for you to pass-in\n",
        "arbitrary expressions that just need to have some aggregation specified"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd4sVrtvTfSW",
        "outputId": "929efeca-6543-4c98-e969-f4b021ae776b"
      },
      "source": [
        "df.groupBy('InvoiceNo').agg(\n",
        "                pyf.count('Quantity').alias('quan'), # use count function\n",
        "                expr('count(Quantity)')).show(3) # count from SQL expression"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----+---------------+\n",
            "|InvoiceNo|quan|count(Quantity)|\n",
            "+---------+----+---------------+\n",
            "|   536596|   6|              6|\n",
            "|   536938|  14|             14|\n",
            "|   537252|   1|              1|\n",
            "+---------+----+---------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4HKNoxMTfSW"
      },
      "source": [
        "### Grouping with Maps\n",
        "\n",
        "series of Maps for which the key\n",
        "is the column, and the value is the aggregation function (as a string)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czLzVVE2TfSW",
        "outputId": "8f4b7d17-4ee5-4d6a-8fd1-e0627de916fa"
      },
      "source": [
        "df.groupBy('InvoiceNo').agg(expr('avg(Quantity)'),expr('stddev_pop(Quantity)'))\\\n",
        ".show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+------------------+--------------------+\n",
            "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
            "+---------+------------------+--------------------+\n",
            "|   536596|               1.5|  1.1180339887498947|\n",
            "|   536938|33.142857142857146|  20.698023172885524|\n",
            "|   537252|              31.0|                 0.0|\n",
            "+---------+------------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7-Wyq6XTfSW"
      },
      "source": [
        "## Window Functions\n",
        "\n",
        "A group-by takes data, and every row can go only into one grouping. A window function\n",
        "calculates a return value for every input row of a table based on a group of rows, called a frame.\n",
        "\n",
        "Spark supports three kinds of window functions: ranking functions, analytic functions,\n",
        "and aggregate functions.\n",
        "\n",
        "Tasks like calculating a moving average, calculating a cumulative sum, or accessing the values of a row appearing before the current row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q-GPS7Xlfo6"
      },
      "source": [
        "**Ranking functions**\n",
        " rank() rank()\n",
        "dense_rank() denseRank()\n",
        "percent_rank() percentRank()\n",
        "ntile() ntile()\n",
        "row_number() rowNumber()  \n",
        "\n",
        "**Analytic functions**\n",
        "cume_dist() cumeDist()\n",
        "first_value() firstValue()\n",
        "last_value() lastValue()\n",
        "lag() lag()\n",
        "lead() lead()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU77pX8wTfSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "990b96d0-50d0-45c4-d97d-ec4a6e6bcf0e"
      },
      "source": [
        "# cast to date spark type\n",
        "from pyspark.sql.functions import col, to_date\n",
        "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
        "dfWithDate.createOrReplaceTempView(\"dfWithDate\")\n",
        "dfWithDate.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- InvoiceNo: string (nullable = true)\n",
            " |-- StockCode: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: string (nullable = true)\n",
            " |-- InvoiceDate: string (nullable = true)\n",
            " |-- UnitPrice: string (nullable = true)\n",
            " |-- CustomerID: string (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JVNj3QOTfSX"
      },
      "source": [
        "The first step is to create a window specification.  \n",
        "The ordering determines\n",
        "the ordering within a given partition.  \n",
        "The frame specification (the rowsBetween\n",
        "statement) states which rows will be included in the frame based on its reference to the current\n",
        "input row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qduEVseMmNCf",
        "outputId": "76634855-1deb-457f-81b1-85c150133af2"
      },
      "source": [
        "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
        "    (\"Michael\", \"Sales\", 4600),  \\\n",
        "    (\"Robert\", \"Sales\", 4100),   \\\n",
        "    (\"Maria\", \"Finance\", 3000),  \\\n",
        "    (\"James\", \"Sales\", 3000),    \\\n",
        "    (\"Scott\", \"Finance\", 3300),  \\\n",
        "    (\"Jen\", \"Finance\", 3900),    \\\n",
        "    (\"Jeff\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000),\\\n",
        "    (\"Saif\", \"Sales\", 4100) \\\n",
        "  )\n",
        " \n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVSPQ2CUmM_I",
        "outputId": "ef1fb33c-d324-4627-d47a-1319cb56c599"
      },
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "\n",
        "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
        "    .withColumn('rank', rank().over(windowSpec))\\\n",
        "    .show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----------+----+\n",
            "|employee_name|department|salary|row_number|rank|\n",
            "+-------------+----------+------+----------+----+\n",
            "|James        |Sales     |3000  |1         |1   |\n",
            "|James        |Sales     |3000  |2         |1   |\n",
            "|Robert       |Sales     |4100  |3         |3   |\n",
            "|Saif         |Sales     |4100  |4         |3   |\n",
            "|Michael      |Sales     |4600  |5         |5   |\n",
            "|Maria        |Finance   |3000  |1         |1   |\n",
            "|Scott        |Finance   |3300  |2         |2   |\n",
            "|Jen          |Finance   |3900  |3         |3   |\n",
            "|Kumar        |Marketing |2000  |1         |1   |\n",
            "|Jeff         |Marketing |3000  |2         |2   |\n",
            "+-------------+----------+------+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAj5He7vnb09",
        "outputId": "ce3f41a8-c3b8-432a-da07-e16401da2479"
      },
      "source": [
        "# You can define window column and use it later in select\n",
        "max_salary = max(col('salary')).over(windowSpec)\n",
        "rank_salary = dense_rank().over(windowSpec)\n",
        "\n",
        "df.select(\n",
        "    F.expr('*')\n",
        "    ,max_salary.alias('max_salary')\n",
        "    ,rank_salary.alias('ranking')\n",
        ").show(truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----------+-------+\n",
            "|employee_name|department|salary|max_salary|ranking|\n",
            "+-------------+----------+------+----------+-------+\n",
            "|James        |Sales     |3000  |3000      |1      |\n",
            "|James        |Sales     |3000  |3000      |1      |\n",
            "|Robert       |Sales     |4100  |4100      |2      |\n",
            "|Saif         |Sales     |4100  |4100      |2      |\n",
            "|Michael      |Sales     |4600  |4600      |3      |\n",
            "|Maria        |Finance   |3000  |3000      |1      |\n",
            "|Scott        |Finance   |3300  |3300      |2      |\n",
            "|Jen          |Finance   |3900  |3900      |3      |\n",
            "|Kumar        |Marketing |2000  |2000      |1      |\n",
            "|Jeff         |Marketing |3000  |3000      |2      |\n",
            "+-------------+----------+------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2TWhCRKTfSX"
      },
      "source": [
        "## Grouping Sets\n",
        "\n",
        "grouping sets is an aggregation across multiple groups (only available in SQL ).  \n",
        "Grouping sets are a low-level tool for combining sets of aggregations together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4eUQ4miTfSY",
        "outputId": "ae4c9d73-6667-425c-f614-ea27be928ce0"
      },
      "source": [
        "dfNoNull = dfwithDate.drop()\n",
        "dfNoNull.createOrReplaceTempView(\"dfNoNull\")\n",
        "#sample with SQL // Sum quantity by customer id and stockcode\n",
        "spark.sql(\"\"\"\n",
        "SELECT CustomerID, StockCode, sum(Quantity) AS Quantity_by_id_code\n",
        "FROM dfNonull\n",
        "GROUP BY CustomerID, StockCode\n",
        "ORDER BY CustomerId DESC, StockCode DESC\"\"\").show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---------+-------------------+\n",
            "|CustomerID|StockCode|Quantity_by_id_code|\n",
            "+----------+---------+-------------------+\n",
            "|     18287|    85173|               48.0|\n",
            "|     18287|   85040A|               48.0|\n",
            "|     18287|   85039B|              120.0|\n",
            "|     18287|   85039A|               96.0|\n",
            "|     18287|    84920|                4.0|\n",
            "+----------+---------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O_GxrziTfSY",
        "outputId": "93544998-7f37-4c24-f032-1fddbfc6c827"
      },
      "source": [
        "# achieve same result using grouping set // need to filter out null value\n",
        "spark.sql(\"\"\"\n",
        "SELECT CustomerID, StockCode, sum(Quantity) AS Quantity_by_id_code\n",
        "FROM dfNonull\n",
        "GROUP BY GROUPING SETS((CustomerId,StockCode)) \n",
        "ORDER BY CustomerId DESC, StockCode DESC\"\"\").show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---------+-------------------+\n",
            "|CustomerId|StockCode|Quantity_by_id_code|\n",
            "+----------+---------+-------------------+\n",
            "|     18287|    85173|               48.0|\n",
            "|     18287|   85040A|               48.0|\n",
            "|     18287|   85039B|              120.0|\n",
            "|     18287|   85039A|               96.0|\n",
            "|     18287|    84920|                4.0|\n",
            "+----------+---------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dul3eGpOTfSY"
      },
      "source": [
        "to include the total number of items, regardless of\n",
        "customer or stock code, you can union of several different groupings together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcgmPkKdTfSY",
        "outputId": "53f24ca9-e52a-46cc-81fb-1cb5b8e80aa4"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT CustomerID, StockCode, sum(Quantity) AS Quantity_by_id_code\n",
        "FROM dfNonull\n",
        "GROUP BY GROUPING SETS( (CustomerId,StockCode),() ) \n",
        "ORDER BY CustomerId DESC, StockCode DESC\"\"\").show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---------+-------------------+\n",
            "|CustomerId|StockCode|Quantity_by_id_code|\n",
            "+----------+---------+-------------------+\n",
            "|     18287|    85173|               48.0|\n",
            "|     18287|   85040A|               48.0|\n",
            "|     18287|   85039B|              120.0|\n",
            "|     18287|   85039A|               96.0|\n",
            "|     18287|    84920|                4.0|\n",
            "+----------+---------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-KE3I6wTfSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c0a56e-493e-4d54-bcf7-681579d99332"
      },
      "source": [
        "# a better example is \n",
        "dfbrand = spark.createDataFrame([('LV', 'M',10), ('Uniqlo', 'S',5), ('LV', 'L',3), (\"Uniqlo\", 'S',8)],('Brand','Size','Sales'))\n",
        "dfbrand.createOrReplaceTempView(\"dfbrand\")\n",
        "dfbrand.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+-----+\n",
            "| Brand|Size|Sales|\n",
            "+------+----+-----+\n",
            "|    LV|   M|   10|\n",
            "|Uniqlo|   S|    5|\n",
            "|    LV|   L|    3|\n",
            "|Uniqlo|   S|    8|\n",
            "+------+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp_nT4xMTfSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3021d4a-08eb-4906-8eb6-8d51cd379cf5"
      },
      "source": [
        "# the result is union of each group where () = no group at all\n",
        "spark.sql(\"\"\"\n",
        "SELECT Brand, Size,sum(Sales)\n",
        "FROM dfbrand\n",
        "GROUP BY GROUPING SETS(   (Brand,Size) ,(Size) , ()   )\n",
        "ORDER BY BRAND desc, SIZE DESC\n",
        "\"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+----------+\n",
            "| Brand|Size|sum(Sales)|\n",
            "+------+----+----------+\n",
            "|Uniqlo|   S|        13|\n",
            "|    LV|   M|        10|\n",
            "|    LV|   L|         3|\n",
            "|  null|   S|        13|\n",
            "|  null|   M|        10|\n",
            "|  null|   L|         3|\n",
            "|  null|null|        26|\n",
            "+------+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPlpejLiTfSY"
      },
      "source": [
        "Row 1-3 is Grouping set by (Brand,size)  \n",
        "Row 4-6 is Grouping set by (size) regardless of brand, etc  \n",
        "Row 7 is Grouping set by none (just sum all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz83lpz4TfSY"
      },
      "source": [
        "## Rollups\n",
        "\n",
        "create a rollup(multidimensional aggregation) that looks across time (with our new Date column) and space (with the\n",
        "Country column) and   \n",
        "creates a new DataFrame that includes the grand total over all dates, the\n",
        "grand total for each date in the DataFrame, and the subtotal for each country on each date in the\n",
        "DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4q4WTxDTfSZ",
        "outputId": "37336f36-b6b0-423d-94e2-7953b2bdb668"
      },
      "source": [
        "rolledUpDF = dfNoNull.rollup('Date','Country').agg(pyf.sum('Quantity'))\\\n",
        "    .selectExpr('Date','Country',\" `sum(Quantity)` as  total_quantity\").orderBy('Date')\n",
        "rolledUpDF.show(8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------------+--------------+\n",
            "|      Date|       Country|total_quantity|\n",
            "+----------+--------------+--------------+\n",
            "|      null|          null|     5176450.0|\n",
            "|2010-12-01|United Kingdom|       23949.0|\n",
            "|2010-12-01|          EIRE|         243.0|\n",
            "|2010-12-01|     Australia|         107.0|\n",
            "|2010-12-01|        France|         449.0|\n",
            "|2010-12-01|          null|       26814.0|\n",
            "|2010-12-01|       Germany|         117.0|\n",
            "|2010-12-01|        Norway|        1852.0|\n",
            "+----------+--------------+--------------+\n",
            "only showing top 8 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTNZO6ZaTfSZ"
      },
      "source": [
        "Now where you see the null values is where you’ll find the grand totals. A null in both rollup\n",
        "columns specifies the grand total across both of those columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuaYR9rrTfSZ",
        "outputId": "4886ad02-f6a7-4900-f8a0-2a28a4ae5def"
      },
      "source": [
        "# Total by date\n",
        "rolledUpDF.where(\"Country IS NULL\").show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------+--------------+\n",
            "|      Date|Country|total_quantity|\n",
            "+----------+-------+--------------+\n",
            "|      null|   null|     5176450.0|\n",
            "|2010-12-01|   null|       26814.0|\n",
            "|2010-12-02|   null|       21023.0|\n",
            "|2010-12-03|   null|       14830.0|\n",
            "|2010-12-05|   null|       16395.0|\n",
            "+----------+-------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGY2UjMUTfSZ",
        "outputId": "07afc520-1df6-4ef9-8393-ac7df1b568b9"
      },
      "source": [
        "# total for all\n",
        "rolledUpDF.where(\"Date IS NULL\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-------+--------------+\n",
            "|Date|Country|total_quantity|\n",
            "+----+-------+--------------+\n",
            "|null|   null|     5176450.0|\n",
            "+----+-------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QACUjwjTfSZ"
      },
      "source": [
        "## Cube\n",
        "A cube takes the rollup to a level deeper. Rather than treating elements hierarchically, a cube\n",
        "does the same thing across all dimensions. This means that it won’t just go by date over the\n",
        "entire time period, but also the country"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw0yQd1UTfSZ",
        "outputId": "f3cf342e-b7b6-4ffc-abcd-a9fbe5643e65"
      },
      "source": [
        "dfNoNull.cube(\"Date\", \"Country\").agg(pyf.sum(col(\"Quantity\")))\\\n",
        "    .select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"Date\").show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+------------------+-------------+\n",
            "|Date|           Country|sum(Quantity)|\n",
            "+----+------------------+-------------+\n",
            "|null|             Spain|      26824.0|\n",
            "|null|           Denmark|       8188.0|\n",
            "|null|    Czech Republic|        592.0|\n",
            "|null|European Community|        497.0|\n",
            "|null|            Norway|      19247.0|\n",
            "+----+------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvNOuZ4WTfSZ",
        "outputId": "578aaf05-99bc-4a8b-dd63-1f1ec8549b54"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "# more visualize example\n",
        "studentsDF = spark.createDataFrame([\n",
        "          (\"mario\", \"italy\", \"europe\",5),\n",
        "          (\"stefano\", \"italy\", \"europe\",8),\n",
        "          (\"victor\", \"spain\", \"europe\",8),\n",
        "          (\"li\", \"china\", \"asia\",10),\n",
        "          (\"yuki\", \"japan\", \"asia\",5),\n",
        "          (\"vito\", \"italy\", \"europe\",3)\n",
        "], [\"name\", \"country\", \"continent\",'sales'])\n",
        "#\n",
        "studentsDF.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------+---------+-----+\n",
            "|   name|country|continent|sales|\n",
            "+-------+-------+---------+-----+\n",
            "|  mario|  italy|   europe|    5|\n",
            "|stefano|  italy|   europe|    8|\n",
            "| victor|  spain|   europe|    8|\n",
            "|     li|  china|     asia|   10|\n",
            "|   yuki|  japan|     asia|    5|\n",
            "|   vito|  italy|   europe|    3|\n",
            "+-------+-------+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtA8RgDOTfSa",
        "outputId": "7e70f280-7007-4727-f5fb-99f9d8d0efe2"
      },
      "source": [
        "# summarize on all dimension\n",
        "studentsDF.cube('country','continent').agg(pyf.sum(col('sales'))).orderBy('country','continent').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---------+----------+\n",
            "|country|continent|sum(sales)|\n",
            "+-------+---------+----------+\n",
            "|   null|     null|        39|\n",
            "|   null|     asia|        15|\n",
            "|   null|   europe|        24|\n",
            "|  china|     null|        10|\n",
            "|  china|     asia|        10|\n",
            "|  italy|     null|        16|\n",
            "|  italy|   europe|        16|\n",
            "|  japan|     null|         5|\n",
            "|  japan|     asia|         5|\n",
            "|  spain|     null|         8|\n",
            "|  spain|   europe|         8|\n",
            "+-------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqcLdFkwTfSa"
      },
      "source": [
        "Row 1 =  Grand total  \n",
        "Row 2-3 = Sum by continent regardless of country  \n",
        "Row 4-11 = sum by country regardless of continent "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "venN40XoTfSa"
      },
      "source": [
        "## Grouping Metadata\n",
        "\n",
        "using the grouping_id,\n",
        "which gives us a column specifying the level of aggregation that we have in our result set.\n",
        "\n",
        "Grouping ID\n",
        "3 -> highest level aggregation give the total quantity  \n",
        "2 - > total quantity per stock code regardless of customer  \n",
        "1-> total quantity on a per - customer basis regardless of item  \n",
        "0 - > total quantity for individual customer id and stock combind "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7YQnE5bTfSa",
        "outputId": "22a7aa58-c20f-4e1c-adb9-07ee80d59c09"
      },
      "source": [
        "dfNoNull.cube(\"customerId\", \"stockCode\").agg(pyf.grouping_id(), pyf.sum(\"Quantity\"))\\\n",
        ".orderBy(col('CustomerId').desc())\\\n",
        ".show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---------+-------------+-------------+\n",
            "|customerId|stockCode|grouping_id()|sum(Quantity)|\n",
            "+----------+---------+-------------+-------------+\n",
            "|     18287|    23378|            0|         24.0|\n",
            "|     18287|    20961|            0|         30.0|\n",
            "|     18287|    21556|            0|         12.0|\n",
            "|     18287|   85040A|            0|         48.0|\n",
            "|     18287|    22306|            0|         24.0|\n",
            "+----------+---------+-------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olVXwjUzTfSa"
      },
      "source": [
        "## Pivot\n",
        "Convert row into column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8fkfV3z3kEO",
        "outputId": "56536a80-f852-4f2b-f60c-603a8b26f366"
      },
      "source": [
        "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
        "    (\"Michael\", \"Sales\", 4600),  \\\n",
        "    (\"Robert\", \"Sales\", 4100),   \\\n",
        "    (\"Maria\", \"Finance\", 3000),  \\\n",
        "    (\"James\", \"Sales\", 3000),    \\\n",
        "    (\"Scott\", \"Finance\", 3300),  \\\n",
        "    (\"Jen\", \"Finance\", 3900),    \\\n",
        "    (\"Jeff\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000),\\\n",
        "    (\"Saif\", \"Sales\", 4100) \\\n",
        "  )\n",
        " \n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEtXmA023oAu",
        "outputId": "afec67a4-43a6-4223-eef1-9cb03bd0a079"
      },
      "source": [
        "pivot_table = df.groupby(\"department\").pivot(\"employee_name\").sum()\n",
        "pivot_table.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----+----+-----+-----+-------+------+----+-----+\n",
            "|department|James|Jeff| Jen|Kumar|Maria|Michael|Robert|Saif|Scott|\n",
            "+----------+-----+----+----+-----+-----+-------+------+----+-----+\n",
            "|     Sales| 6000|null|null| null| null|   4600|  4100|4100| null|\n",
            "|   Finance| null|null|3900| null| 3000|   null|  null|null| 3300|\n",
            "| Marketing| null|3000|null| 2000| null|   null|  null|null| null|\n",
            "+----------+-----+----+----+-----+-----+-------+------+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuJt5IocTfSb"
      },
      "source": [
        "## User-Defined Aggregation Functions\n",
        "\n",
        "You can use UDAFs to compute custom\n",
        "calculations over groups of input data (as opposed to single rows). Spark maintains a single\n",
        "AggregationBuffer to store intermediate results for every group of input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H779DjaxTfSb",
        "outputId": "f4faea76-b3de-4503-d098-fddf809a5132"
      },
      "source": [
        "a = sc.parallelize([[1, 1, 'aa'],\n",
        "                    [1, 2, 'a'],\n",
        "                    [1, 1, 'ba'],\n",
        "                    [1, 2, 'b'],\n",
        "                    [2, 1, 'cc']]).toDF(['id', 'value1', 'value2'])\n",
        "a.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+\n",
            "| id|value1|value2|\n",
            "+---+------+------+\n",
            "|  1|     1|    aa|\n",
            "|  1|     2|     a|\n",
            "|  1|     1|    ba|\n",
            "|  1|     2|     b|\n",
            "|  2|     1|    cc|\n",
            "+---+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBU-0wvCTfSb",
        "outputId": "f3e0f024-598d-481a-dafd-bc028b967378"
      },
      "source": [
        "def find_a(x):\n",
        "  \"\"\"Count occurance 'a's in list.\"\"\"\n",
        "  output_count = 0\n",
        "  for i in x:\n",
        "    if i == 'a':\n",
        "      output_count += 1\n",
        "  return output_count\n",
        "\n",
        "find_a_udf = udf(find_a, IntegerType())\n",
        "\n",
        "a.groupBy('id').agg(find_a_udf(pyf.collect_list('value2')).alias('a_count')).show() # only match exact 'a' not 'aa'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+\n",
            "| id|a_count|\n",
            "+---+-------+\n",
            "|  1|      1|\n",
            "|  2|      0|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygQwCl49TfSb"
      },
      "source": [
        "# Part 8 Joins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jPf9kkFTfSb"
      },
      "source": [
        "# create sample table to use \n",
        "person = spark.createDataFrame([\n",
        "    (0, \"Bill Chambers\", 0, [100]), # row 1 \n",
        "    (1, \"Matei Zaharia\", 1, [500, 250, 100]), # row 2\n",
        "    (2, \"Michael Armbrust\", 1, [250, 100])\n",
        "]).toDF('student_id','name','graduate_program_id',\"spark_status\") # specify column name\n",
        "\n",
        "graduateProgram = spark.createDataFrame([\n",
        "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
        "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
        "    (1, \"Ph.D.\", \"Statistic\", \"UChicago\")\n",
        "]).toDF(\"id\", \"degree\", \"department\", \"school\")\n",
        "\n",
        "sparkStatus = spark.createDataFrame([\n",
        "    (500, \"Vice President\"),\n",
        "    (250, \"PMC Member\"),\n",
        "    (100, \"Contributor\")\n",
        "]).toDF(\"id\", \"status\")\n",
        "\n",
        "#Register in SQL\n",
        "person.createOrReplaceTempView(\"person\")\n",
        "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
        "sparkStatus.createOrReplaceTempView(\"sparkStatus\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixXVziYLTfSb"
      },
      "source": [
        "## Inner Joins (default)\n",
        "evaluate the keys in both of the DataFrames and include\n",
        "only the rows that evaluate to true.  \n",
        "So we have to express keys for Spark to evaluate\n",
        "\n",
        "API = 'inner', default  \n",
        "SQL = JOIN, INNER JOIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvoFvM1NTfSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be4ca395-1961-4e58-b1d9-5a20fa9da0c1"
      },
      "source": [
        "# 1. express keys for Spark to evaluate\n",
        "joinExpression = person['graduate_program_id'] == graduateProgram['id']\n",
        "# 2.join\n",
        "person.join(graduateProgram,joinExpression,'inner').show() #left.join(right,expression,'how')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------+-------------------+---------------+---+-------+--------------------+-----------+\n",
            "|student_id|            name|graduate_program_id|   spark_status| id| degree|          department|     school|\n",
            "+----------+----------------+-------------------+---------------+---+-------+--------------------+-----------+\n",
            "|         0|   Bill Chambers|                  0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
            "|         1|   Matei Zaharia|                  1|[500, 250, 100]|  1|  Ph.D.|           Statistic|   UChicago|\n",
            "|         2|Michael Armbrust|                  1|     [250, 100]|  1|  Ph.D.|           Statistic|   UChicago|\n",
            "+----------+----------------+-------------------+---------------+---+-------+--------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUePv3FVTfSe",
        "outputId": "b9e34227-5e4e-451d-93bf-4f846fe0b8ea"
      },
      "source": [
        "# equal to this spark.SQL\n",
        "spark.sql(\"\"\"\n",
        "SELECT *\n",
        "FROM PERSON P\n",
        "JOIN graduateProgram G on P.graduate_program_id = G.ID \"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------------+-------------------+---------------+---+-------+--------------------+-----------+\n",
            "|student_id|            name|graduate_program_id|   spark_status| id| degree|          department|     school|\n",
            "+----------+----------------+-------------------+---------------+---+-------+--------------------+-----------+\n",
            "|         0|   Bill Chambers|                  0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
            "|         1|   Matei Zaharia|                  1|[500, 250, 100]|  1|  Ph.D.|           Statistic|   UChicago|\n",
            "|         2|Michael Armbrust|                  1|     [250, 100]|  1|  Ph.D.|           Statistic|   UChicago|\n",
            "+----------+----------------+-------------------+---------------+---+-------+--------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK0H9pYyTfSe"
      },
      "source": [
        "## Outer Joins (Full outer joins)\n",
        "evaluate the keys in both of the DataFrames and includes the rows that evaluate to true or false. If there is no equivalent row in either the left or\n",
        "right DataFrame, Spark will insert null:  \n",
        "\n",
        "API = 'outer'  \n",
        "SQL = FULL OUTER JOIN or OUTER JOIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBJqN2v2TfSe",
        "outputId": "46bcdc09-2f87-41d8-ccae-0bfd41440604"
      },
      "source": [
        "person.join(graduateProgram,joinExpression,'outer').show() # program id 2 cant match and return null for empty field"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------------+-------------------+---------------+---+-------+--------------------+-----------+\n",
            "|student_id|            name|graduate_program_id|   spark_status| id| degree|          department|     school|\n",
            "+----------+----------------+-------------------+---------------+---+-------+--------------------+-----------+\n",
            "|         0|   Bill Chambers|                  0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
            "|         1|   Matei Zaharia|                  1|[500, 250, 100]|  1|  Ph.D.|           Statistic|   UChicago|\n",
            "|         2|Michael Armbrust|                  1|     [250, 100]|  1|  Ph.D.|           Statistic|   UChicago|\n",
            "|      null|            null|               null|           null|  2|Masters|                EECS|UC Berkeley|\n",
            "+----------+----------------+-------------------+---------------+---+-------+--------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrPVmcpCTfSf"
      },
      "source": [
        "## Left Join (Left Join Outer JOIN)\n",
        "Left outer joins evaluate the keys in both of the DataFrames or tables and includes all rows from\n",
        "the left DataFrame as well as any rows in the right DataFrame that must have a match in the left\n",
        "DataFrame\n",
        "\n",
        "API = 'left' or 'left_outer'  \n",
        "SQL = LEFT JOIN or LEFT OUTER JOIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s29CFpeOTfSf",
        "outputId": "8fd5ecf4-7975-4321-ef29-b1cc33882c73"
      },
      "source": [
        "graduateProgram.join(person,joinExpression,'left_outer').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+--------------------+-----------+----------+----------------+-------------------+---------------+\n",
            "| id| degree|          department|     school|student_id|            name|graduate_program_id|   spark_status|\n",
            "+---+-------+--------------------+-----------+----------+----------------+-------------------+---------------+\n",
            "|  0|Masters|School of Informa...|UC Berkeley|         0|   Bill Chambers|                  0|          [100]|\n",
            "|  1|  Ph.D.|           Statistic|   UChicago|         1|   Matei Zaharia|                  1|[500, 250, 100]|\n",
            "|  1|  Ph.D.|           Statistic|   UChicago|         2|Michael Armbrust|                  1|     [250, 100]|\n",
            "|  2|Masters|                EECS|UC Berkeley|      null|            null|               null|           null|\n",
            "+---+-------+--------------------+-----------+----------+----------------+-------------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2sPGUW-TfSf"
      },
      "source": [
        "## Right Joins (Right outer join)\n",
        "evaluate the keys in both of the DataFrames or tables and includes all rows\n",
        "from the right DataFrame as well as any rows in the left DataFrame that have a match in the right\n",
        "\n",
        "API = 'right' or 'right_outer'  \n",
        "SQL = RIGHT JOIN  or RIGHT OUTER JOIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWsufz-DTfSf"
      },
      "source": [
        "## Left Semi Joins\n",
        "\n",
        "**Inner join but return only data from the left table** (Return data in table left that has key match in table right)   \n",
        "API = 'left_semi'  \n",
        "SQL = LEFT SEMI JOIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD_Fb7d9TfSf",
        "outputId": "2926f613-9cad-461a-fea6-d829a8b61d65"
      },
      "source": [
        "graduateProgram.join(person, joinExpression, 'left_semi').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+--------------------+-----------+\n",
            "| id| degree|          department|     school|\n",
            "+---+-------+--------------------+-----------+\n",
            "|  0|Masters|School of Informa...|UC Berkeley|\n",
            "|  1|  Ph.D.|           Statistic|   UChicago|\n",
            "+---+-------+--------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh1FKDLxTfSf"
      },
      "source": [
        "## Left Anti Joins\n",
        "\n",
        "Return only the data from left table that keys not in the right table (Simialr to NOT IN)\n",
        "\n",
        "DPI = 'left_anti'\n",
        "SQL = NOT IN , LEFT ANTI JOIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti97wBT4TfSf",
        "outputId": "da78761c-ba4c-4abf-e435-443363deda77"
      },
      "source": [
        "graduateProgram.join(person, joinExpression, 'left_anti').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+----------+-----------+\n",
            "| id| degree|department|     school|\n",
            "+---+-------+----------+-----------+\n",
            "|  2|Masters|      EECS|UC Berkeley|\n",
            "+---+-------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RZoQlETTfSf",
        "outputId": "997a522b-57fd-4e97-ac2a-578c276d448f"
      },
      "source": [
        "# Same thing as not in below\n",
        "spark.sql(\"\"\"\n",
        "select *\n",
        "FROM graduateProgram \n",
        "WHERE graduateProgram.id not in (SELECT graduate_program_id\n",
        "                                FROM person)\n",
        "\"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+----------+-----------+\n",
            "| id| degree|department|     school|\n",
            "+---+-------+----------+-----------+\n",
            "|  2|Masters|      EECS|UC Berkeley|\n",
            "+---+-------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb_TzLwJTfSg"
      },
      "source": [
        "## Join on complex type (Join item in the list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWNnyfhTTfSg"
      },
      "source": [
        "Join id which in the list in the spark_status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY8NPxXZTfSg",
        "outputId": "52648796-68f6-4d4e-e2ab-6603853ed4c2"
      },
      "source": [
        "# column id populate from spark_status\n",
        "person.join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------------+-------------------+---------------+---+--------------+\n",
            "|student_id|            name|graduate_program_id|   spark_status| id|        status|\n",
            "+----------+----------------+-------------------+---------------+---+--------------+\n",
            "|         0|   Bill Chambers|                  0|          [100]|100|   Contributor|\n",
            "|         1|   Matei Zaharia|                  1|[500, 250, 100]|500|Vice President|\n",
            "|         1|   Matei Zaharia|                  1|[500, 250, 100]|250|    PMC Member|\n",
            "|         1|   Matei Zaharia|                  1|[500, 250, 100]|100|   Contributor|\n",
            "|         2|Michael Armbrust|                  1|     [250, 100]|250|    PMC Member|\n",
            "|         2|Michael Armbrust|                  1|     [250, 100]|100|   Contributor|\n",
            "+----------+----------------+-------------------+---------------+---+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pw8j_-YTfSg"
      },
      "source": [
        "## Handling Duplicate Column Names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pffgp7J2TfSg"
      },
      "source": [
        "If two key column from 2 tables have the same name. Result will be error when reference, better to change name one of the column or dropping the column after join"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzU3p9VATfSg",
        "outputId": "be9f16d0-5b89-41c5-9e4d-c67572bfb76d"
      },
      "source": [
        "# set up the same name column\n",
        "person2 = person.withColumnRenamed('graduate_program_id','id')\n",
        "\n",
        "# we drop duplicate id after join\n",
        "joinExpr = graduateProgram['id'] == person2['id']\n",
        "person2.join(graduateProgram,joinExpr).drop(graduateProgram['id']).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------------+---+---------------+-------+--------------------+-----------+\n",
            "|student_id|            name| id|   spark_status| degree|          department|     school|\n",
            "+----------+----------------+---+---------------+-------+--------------------+-----------+\n",
            "|         0|   Bill Chambers|  0|          [100]|Masters|School of Informa...|UC Berkeley|\n",
            "|         1|   Matei Zaharia|  1|[500, 250, 100]|  Ph.D.|           Statistic|   UChicago|\n",
            "|         2|Michael Armbrust|  1|     [250, 100]|  Ph.D.|           Statistic|   UChicago|\n",
            "+----------+----------------+---+---------------+-------+--------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9Pcn9-7TfSh",
        "outputId": "270c4aaf-dea3-4ceb-d912-50ed3c7de4e1"
      },
      "source": [
        "# join on string instad of boolean expression\n",
        "person2.join(graduateProgram,'id').show() # must be same name to be able to directly express in string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------+----------------+---------------+-------+--------------------+-----------+\n",
            "| id|student_id|            name|   spark_status| degree|          department|     school|\n",
            "+---+----------+----------------+---------------+-------+--------------------+-----------+\n",
            "|  0|         0|   Bill Chambers|          [100]|Masters|School of Informa...|UC Berkeley|\n",
            "|  1|         1|   Matei Zaharia|[500, 250, 100]|  Ph.D.|           Statistic|   UChicago|\n",
            "|  1|         2|Michael Armbrust|     [250, 100]|  Ph.D.|           Statistic|   UChicago|\n",
            "+---+----------+----------------+---------------+-------+--------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkyxHUAYTfSh"
      },
      "source": [
        "## Communication Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxl22_Z9TfSh"
      },
      "source": [
        "### Big table–to–big table\n",
        "Spark use *shuffle join* to join a big table to another big table.  \n",
        "This means every node(Executors) talk and share keys with every other node that store partitions of data. (expensive joins)\n",
        "\n",
        "### Big table–to–small table\n",
        "Table is small enough to fit in the memory of single worker node. Spark use *broadcast join* which means replicate small DataFrame onto every worker node in the cluster. This prevent us from performing the all-to-all communication during the entire join process then let each individual worker node perform the\n",
        "work without having to wait or communicate with any other worker nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRvdQ6YXTfSh"
      },
      "source": [
        "# Part 9. Data Sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40twU-eKTfSh"
      },
      "source": [
        "## The Structure of the Data Sources API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dPI4IIjTfSh"
      },
      "source": [
        "### Read API Structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATSQm29CTfSh"
      },
      "source": [
        "# version 1\n",
        "spark.read.format(...).option(\"mode\", \"FAILFAST\").option(\"key\", \"value\").schema(...).load()\n",
        "# version 2\n",
        "spark.read.csv(\n",
        "    datapath\n",
        "    ,schema\n",
        "    ,header\n",
        "    ,mode\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXdKTEY0TfSh"
      },
      "source": [
        "After we have a DataFrame reader, we specify several values:  \n",
        "The format  \n",
        "The schema  \n",
        "The read mode  \n",
        "A series of options  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHYmLdiiTfSi"
      },
      "source": [
        "#### Read modes\n",
        "Read modes specify what will happen\n",
        "when Spark does come across malformed records\n",
        "\n",
        "| Read mode     | Description                                                                                                                              |\n",
        "|---------------|------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| permissive    | Sets all fields to null when it encounters a corrupted record and places all corrupted records in a string column called _corrupt_record |\n",
        "| dropMalformed | Drops the row that contains malformed records                                                                                            |\n",
        "| failFast      | Fails immediately upon encountering malformed records                                                                                    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4QDdKhHTfSi"
      },
      "source": [
        "### Write API Structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuaWgYvaTfSi"
      },
      "source": [
        "DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()\n",
        "# or\n",
        "spark.write.csv(\n",
        "    #...\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lapcf9AoTfSi"
      },
      "source": [
        "#### Save modes\n",
        "\n",
        "| Save mode     | Description                                                                                  |\n",
        "|---------------|----------------------------------------------------------------------------------------------|\n",
        "| append        | Appends the output files to the list of files that already exist at that location            |\n",
        "| overwrite     | Will completely overwrite any data that already exists there                                 |\n",
        "| errorIfExists | Throws an error and fails the write if data or files already exist at the specified location |\n",
        "| ignore        | If data or files exist at the location, do nothing with the current DataFrame                |\n",
        "\n",
        "The default is errorIfExists. This means that if Spark finds data at the location to which\n",
        "you’re writing, it will fail the write immediately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9vJybZdTfSi"
      },
      "source": [
        "## CSV Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVvR7VpxTfSi"
      },
      "source": [
        "### Read CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ_vPXTaTfSi"
      },
      "source": [
        "# csv files\n",
        "spark.read.format(\"csv\")\\\n",
        "            .option(\"header\", \"true\")\\\n",
        "            .option(\"mode\", \"FAILFAST\")\\\n",
        "            .option(\"inferSchema\", \"true\")\\ #Specifies whether Spark should infer column types when reading the file.\n",
        "            .load(\"some/path/to/file.csv\")\\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noDZPG1nTfSi"
      },
      "source": [
        "### Write CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GHOOLlfTfSi"
      },
      "source": [
        "# version 1\n",
        "# read to FileForWrite first\n",
        "csvFile = spark.read.format(\"csv\")\\\n",
        "            .option(\"header\", \"true\")\\\n",
        "            .option(\"mode\", \"FAILFAST\")\\\n",
        "            .option(\"inferSchema\", \"true\")\\\n",
        "            .load(\"data/flight-data/csv/2010-summary.csv\")\n",
        "# then write\n",
        "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").save(\"tmp/my-tsv-file.tsv\")\n",
        "#This actually reflects the number of partitions in our DataFrame at the time we write it out. If"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WWssrlR6a5Z"
      },
      "source": [
        "# version2\n",
        "csvFile = spark.read.csv(\n",
        "    datapath + 'flight-data/csv/2010-summary.csv'\n",
        "    ,header = True\n",
        "    ,mode = \"FAILFAST\"\n",
        "    ,inferSchema = True\n",
        ")\n",
        "# then write\n",
        "csvFile.write.parquet(\n",
        "    datapath + 'tmp/tmkp.parquet'\n",
        "    ,mode = 'overwrite'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP92gXP77HbS",
        "outputId": "7e6d0efc-d497-4a0b-8b9d-a66ef7bb68af"
      },
      "source": [
        "%%shell\n",
        "ls Spark-The-Definitive-Guide/data/tmp/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tmkp.parquet\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWPYWks2UZQ2"
      },
      "source": [
        "## Comparison\n",
        "| Type    | <span style=\"white-space:nowrap\">Inference Type</span> | <span style=\"white-space:nowrap\">Inference Speed</span> | Reason                                          | <span style=\"white-space:nowrap\">Should Supply Schema?</span> |\n",
        "|---------|--------------------------------------------------------|---------------------------------------------------------|----------------------------------------------------|:--------------:|\n",
        "| <b>CSV</b>     | <span style=\"white-space:nowrap\">Full-Data-Read</span> | <span style=\"white-space:nowrap\">Slow</span>            | <span style=\"white-space:nowrap\">File size</span>  | Yes            |\n",
        "| <b>Parquet</b> | <span style=\"white-space:nowrap\">Metadata-Read</span>  | <span style=\"white-space:nowrap\">Fast/Medium</span>     | <span style=\"white-space:nowrap\">Number of Partitions</span> | No (most cases)             |\n",
        "| <b>Tables</b>  | <span style=\"white-space:nowrap\">n/a</span>            | <span style=\"white-space:nowrap\">n/a</span>            | <span style=\"white-space:nowrap\">Predefined</span> | n/a            |\n",
        "| <b>JSON</b>    | <span style=\"white-space:nowrap\">Full-Read-Data</span> | <span style=\"white-space:nowrap\">Slow</span>            | <span style=\"white-space:nowrap\">File size</span>  | Yes            |\n",
        "| <b>Text</b>    | <span style=\"white-space:nowrap\">Dictated</span>       | <span style=\"white-space:nowrap\">Zero</span>            | <span style=\"white-space:nowrap\">Only 1 Column</span>   | Never          |\n",
        "| <b>JDBC</b>    | <span style=\"white-space:nowrap\">DB-Read</span>        | <span style=\"white-space:nowrap\">Fast</span>            | <span style=\"white-space:nowrap\">DB Schema</span>  | No             |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iot8ZwyDUe25"
      },
      "source": [
        "### Reading CSV\n",
        "- `spark.read.csv(..)`\n",
        "- There are a large number of options when reading CSV files including headers, column separator, escaping, etc.\n",
        "- We can allow Spark to infer the schema at the cost of first reading in the entire file.\n",
        "- Large CSV files should always have a schema pre-defined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnbqDEDyUfb4"
      },
      "source": [
        "### Reading Parquet\n",
        "- `spark.read.parquet(..)`\n",
        "- Parquet files are the preferred file format for big-data.\n",
        "- It is a columnar file format.\n",
        "- It is a splittable file format.\n",
        "- It offers a lot of performance benefits over other formats including predicate pushdown.\n",
        "- Unlike CSV, the schema is read in, not inferred.\n",
        "- Reading the schema from Parquet's metadata can be extremely efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxK2pus2Uffq"
      },
      "source": [
        "### Reading Tables\n",
        "- `spark.read.table(..)`\n",
        "- The Databricks platform allows us to register a huge variety of data sources as tables via the Databricks UI.\n",
        "- Any `DataFrame` (from CSV, Parquet, whatever) can be registered as a temporary view.\n",
        "- Tables/Views can be loaded via the `DataFrameReader` to produce a `DataFrame`\n",
        "- Tables/Views can be used directly in SQL statements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeyqF_f9Ufjv"
      },
      "source": [
        "### Reading JSON\n",
        "- `spark.read.json(..)`\n",
        "- JSON represents complex data types unlike CSV's flat format.\n",
        "- Has many of the same limitations as CSV (needing to read the entire file to infer the schema)\n",
        "- Like CSV has a lot of options allowing control on date formats, escaping, single vs. multiline JSON, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEJlRb83UfoM"
      },
      "source": [
        "### Reading Text\n",
        "- `spark.read.text(..)`\n",
        "- Reads one line of text as a single column named `value`.\n",
        "- Is the basis for more complex file formats such as fixed-width text files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIFCP5uqUfry"
      },
      "source": [
        "### Reading JDBC\n",
        "- `spark.read.jdbc(..)`\n",
        "- Requires one database connection per partition.\n",
        "- Has the potential to overwhelm the database.\n",
        "- Requires specification of a stride to properly balance partitions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoAfnqC-TfSi"
      },
      "source": [
        "## JSON file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIHA1Sa8TfSj"
      },
      "source": [
        "Read-in types:  \n",
        "\n",
        "MultiLine. When you set this option to true, you can read an entire file as one dataframe  \n",
        "Line-delimited JSON is actually a much more stable format because it allows you to append to a file with a new record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCHjNrd1TfSj"
      },
      "source": [
        "### Reading JSON Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xs0hfddTfSj",
        "outputId": "434f0c41-e60f-494a-bce5-71e7254bfc2f"
      },
      "source": [
        "spark.read.format('json').option('mode','failfast').option('inferSchema','true')\\\n",
        ".load(\"data/flight-data/json/2010-summary.json\").show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|    1|\n",
            "|    United States|            Ireland|  264|\n",
            "|    United States|              India|   69|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FgRSUCETfSj"
      },
      "source": [
        "### Writing JSON Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E30-xaP7TfSj"
      },
      "source": [
        "#reuse the CSV DataFrame that we created earlier to be the source for our JSON file\n",
        "csvFile.write.format(\"json\").mode(\"overwrite\").save(\"tmp/my-json-file.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvK5gFPKTfSj"
      },
      "source": [
        "## Parquet Files\n",
        "\n",
        "Parquet is an open source column-oriented data store that provides a variety of storage\n",
        "optimizations, especially for analytics workloads.  \n",
        "We\n",
        "recommend writing data out to Parquet for long-term storage because reading from a Parquet file\n",
        "will always be more efficient than JSON or CSV. Another advantage of Parquet is that it\n",
        "supports complex types(array,map, or struct)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZQtFhvMTfSj"
      },
      "source": [
        "### Reading Parquet Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OORl2snRTfSk",
        "outputId": "300d2dad-28f5-48a1-c547-830dde1f8e97"
      },
      "source": [
        "spark.read.format(\"parquet\")\\\n",
        ".load(\"data/flight-data/parquet/2010-summary.parquet\").show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|    1|\n",
            "|    United States|            Ireland|  264|\n",
            "|    United States|              India|   69|\n",
            "|            Egypt|      United States|   24|\n",
            "|Equatorial Guinea|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thZjVuOCTfSk"
      },
      "source": [
        "### Writing Parquet Files "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAKW-WgbTfSk"
      },
      "source": [
        "csvFile.write.format(\"parquet\").mode(\"overwrite\")\\\n",
        ".save(\"tmp/my-parquet-file.parquet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5YI6WTaTfSk"
      },
      "source": [
        "## SQL Databases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGFE7G6CTfSk"
      },
      "source": [
        "# defined the connection properties\n",
        "driver = \"org.sqlite.JDBC\"\n",
        "path = \"data/flight-data/jdbc/my-sqlite.db\"\n",
        "url = \"jdbc:sqlite:\" + path\n",
        "tablename = \"flight_info\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bByH2e2oTfSk"
      },
      "source": [
        "dbDataFrame = spark.read.format(\"jdbc\").option(\"url\", url)\\\n",
        ".option(\"dbtable\", tablename).option(\"driver\", driver).load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp6IWaSDTfSk"
      },
      "source": [
        "# pass SQL query down \n",
        "pushdownQuery = \"\"\"(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info) AS flight_info\"\"\"\n",
        "dbDataFrame = spark.read.format(\"jdbc\")\\\n",
        ".option(\"url\", url).option(\"dbtable\", pushdownQuery).option(\"driver\", driver)\\\n",
        ".load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyTDrz6kTfSk"
      },
      "source": [
        "### Reading from databases in parallel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um6lwXIkTfSl"
      },
      "source": [
        "Specify the ability to specify a maximum number of\n",
        "partitions to allow you to limit how much you are reading and writing in parallel:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFuiMeo-TfSl"
      },
      "source": [
        "dbDataFrame = spark.read.format(\"jdbc\")\\\n",
        ".option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\", driver)\\\n",
        ".option(\"numPartitions\", 10).load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_98RJ-4ATfSl"
      },
      "source": [
        "we can also go further by having them arrive in their own partitions\n",
        "in Spark. We do that by specifying a list of predicates when we create the data source:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5RNxXTMTfSl"
      },
      "source": [
        "props = {\"driver\":\"org.sqlite.JDBC\"}\n",
        "predicates = [\n",
        "            \"DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden'\",\n",
        "            \"DEST_COUNTRY_NAME = 'Anguilla' OR ORIGIN_COUNTRY_NAME = 'Anguilla'\"]\n",
        "spark.read.jdbc(url, tablename, predicates=predicates, properties=props).show()\n",
        "spark.read.jdbc(url,tablename,predicates=predicates,properties=props)\\\n",
        ".rdd.getNumPartitions() # 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32KUz4acTfSl"
      },
      "source": [
        "### Partitioning based on a sliding window\n",
        "\n",
        "Spark then queries our database in parallel and returns numPartitions partitions.\n",
        "We simply modify the upper and lower bounds in order to place certain values in certain\n",
        "partitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wor1hywtTfSl"
      },
      "source": [
        "# set up\n",
        "colName = \"count\"\n",
        "lowerBound = 0\n",
        "upperBound = 348113 # this is the max count in our database\n",
        "numPartitions = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25_HQZbqTfSl"
      },
      "source": [
        "# This will distribute the intervals equally from low to high:\n",
        "spark.read.jdbc(url, tablename, column=colName, properties=props,\n",
        "                lowerBound=lowerBound, upperBound=upperBound,\n",
        "                numPartitions=numPartitions).count() # 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkgTEEeyTfSl"
      },
      "source": [
        "### Writing to SQL Databases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UICdmUqdTfSl"
      },
      "source": [
        "newPath = \"jdbc:sqlite://tmp/my-sqlite.db\"\n",
        "csvFile.write.jdbc(newPath, tablename, mode=\"overwrite\", properties=props)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1IvU8pATfSm"
      },
      "source": [
        "## Text file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZCovrqxTfSm"
      },
      "source": [
        "### Read text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy9bmzms8FL1",
        "outputId": "7baf3695-19a0-409d-fd10-6933080938b2"
      },
      "source": [
        "spark.read.text(datapath + \"flight-data/csv/2010-summary.csv\")\\\n",
        ".selectExpr('*').show(4,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------+\n",
            "|value                                      |\n",
            "+-------------------------------------------+\n",
            "|DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count|\n",
            "|United States,Romania,1                    |\n",
            "|United States,Ireland,264                  |\n",
            "|United States,India,69                     |\n",
            "+-------------------------------------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4mx5dvzTfSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e657f80-b3ff-41e2-b28f-b65ae8c035b4"
      },
      "source": [
        "spark.read.text(datapath + \"flight-data/csv/2010-summary.csv\")\\\n",
        ".selectExpr(\"split(value, ',') as rows\").show(5,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------+\n",
            "|rows                                           |\n",
            "+-----------------------------------------------+\n",
            "|[DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count]|\n",
            "|[United States, Romania, 1]                    |\n",
            "|[United States, Ireland, 264]                  |\n",
            "|[United States, India, 69]                     |\n",
            "|[Egypt, United States, 24]                     |\n",
            "+-----------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OctZD3byTfSm"
      },
      "source": [
        "### Write text\n",
        "make sure to have only one string column; otherwise, the\n",
        "write will fail:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eYS8HC-TfSm"
      },
      "source": [
        "FileForWrite.select(\"DEST_COUNTRY_NAME\").write.text(\"/tmp/simple-text-file2.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEJ_fmvsTfSm"
      },
      "source": [
        "## Splittable File Types and Compression\n",
        "\n",
        "We recommend Parquet with gzip compression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tulw29GxTfSm"
      },
      "source": [
        "### Writing Data in Parallel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqOHiUgVTfSm"
      },
      "source": [
        "# will end up with five files inside of that folder\n",
        "csvFile.repartition(5).write.format(\"csv\").save(\"/tmp/multiple.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOyckg1kTfSm"
      },
      "source": [
        "### Partitioning\n",
        "Partitioning is a tool that allows you to control what data is stored (and where) as you write it.  \n",
        "Basically encode a column(or date) as a\n",
        "folder allows you to read in only the data relevant to your problem instead of having to scan the complete\n",
        "dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuWjC63YTfSn"
      },
      "source": [
        "csvFile.limit(10).write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\")\\\n",
        ".save(\"tmp/partitioned-files.parquet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "473kOPXoTfSn"
      },
      "source": [
        "### Bucketing\n",
        "\n",
        "This will create a certain number of\n",
        "files and organize our data into those “buckets ID”: can help avoid shuffles later when you go to read the data\n",
        "because data with the same bucket ID will all be grouped together into one physical partition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24uKwTObTfSn"
      },
      "source": [
        "numberBuckets = 10\n",
        "columnToBucketBy = \"count\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnLK0xG8TfSn"
      },
      "source": [
        "csvFile.write.format(\"parquet\").mode(\"overwrite\").bucketBy(numberBuckets, columnToBucketBy).saveAsTable(\"bucketedFiles\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LrByqsfTfSn"
      },
      "source": [
        "**Managing file sizes is an important factor not so much for writing data but reading it later on.**  \n",
        "\n",
        "Too small files(small file problem) : incur significant metadata overhead that you incur\n",
        "managing all of those files.\n",
        "\n",
        "Too large files becomes inefficient to have to read entire blocks of data when you need only a\n",
        "few rows.\n",
        "\n",
        "Limit output file sizes so that you can target an optimum file size with $maxRecordsPerFile$ option and specify a number of your choosing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XcLEa8eTfSn"
      },
      "source": [
        "df.write.option(\"maxRecordsPerFile\", 5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pikqldnKTfSn"
      },
      "source": [
        "# Part 10. Spark SQL\n",
        "\n",
        "Spark SQL is intended to operate as an online analytic processing (OLAP) database, not an online\n",
        "transaction processing (OLTP) database. This means that it is not intended to perform extremely lowlatency\n",
        "queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xucO8EaZTfSn",
        "outputId": "0dd414ea-2a42-459e-a69f-7647ff325d18"
      },
      "source": [
        "spark.sql(\"SELECT 1 + 1 as New_Column\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|New_Column|\n",
            "+----------+\n",
            "|         2|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYEuletQTfSo"
      },
      "source": [
        "Combine SQL and DataFrame API to query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZujLrHYyTfSo"
      },
      "source": [
        "# create view\n",
        "spark.read.json(\"data/flight-data/json/2015-summary.json\")\\\n",
        ".createOrReplaceTempView(\"some_sql_view\") # DF => SQL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTnloJ5aTfSo",
        "outputId": "b2d942d7-2704-4e81-dcfc-8a692c43824d"
      },
      "source": [
        "spark.sql(\"\"\"select *\n",
        "            FROM some_sql_view\"\"\").show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2aI_43kTfSo",
        "outputId": "52af0da7-951f-471c-b575-c8637526ba27"
      },
      "source": [
        "spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, sum(count)\n",
        "            FROM some_sql_view \n",
        "            GROUP BY DEST_COUNTRY_NAME\"\"\")\\\n",
        ".where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\")\\ # Use back tick for sum \n",
        ".count() # SQL => DF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTHdqWCwTfSo"
      },
      "source": [
        "## SparkSQL Thrift JDBC/ODBC Server\n",
        "\n",
        "use case is for a business analyst to connect business intelligence software like Tableau to Spark through Java Database Connectivity (JDBC) interface.\n",
        "\n",
        "either launch $spark-sql$ and directly use sql or wrap sql in spark.sql in programmatic interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHnP5m9CTfSo"
      },
      "source": [
        "## Table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0pdYstrTfSo"
      },
      "source": [
        "### Create tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD6PJhDcTfSo"
      },
      "source": [
        "$USING$ specify format of target. If you do not specify the format, Spark will default to a Hive SerDe configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtfYb5jQTfSo",
        "outputId": "910bc320-eedd-4818-a8f8-98949af81d05"
      },
      "source": [
        "# create table using spark.sql\n",
        "spark.sql(\"\"\"CREATE TABLE flights (\n",
        "DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\n",
        "USING JSON OPTIONS (path 'data/flight-data/json/2015-summary.json')\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "execution_count": 348,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO-ZeiUMTfSp",
        "outputId": "4343c3d1-5f6b-4150-c8f4-82c79d8d2107"
      },
      "source": [
        "# add comments to certain columns in a table\n",
        "spark.sql(\"\"\"CREATE TABLE flights_csv (DEST_COUNTRY_NAME STRING,\n",
        "                                        ORIGIN_COUNTRY_NAME STRING COMMENT \"remember, the US will be most prevalent\",\n",
        "                                        count LONG)\n",
        "USING csv OPTIONS (header true, path 'data/flight-data/csv/2015-summary.csv')\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyazWfo3TfSp",
        "outputId": "35c3eea2-9bea-4e12-a0c6-075c1d853ecd"
      },
      "source": [
        "#query from table flights_csv3\n",
        "spark.sql(\"\"\"select * from flights_csv limit 3\"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|            Ireland|  344|\n",
            "+-----------------+-------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjBe6kb6TfSp"
      },
      "source": [
        "# create partitioned table\n",
        "spark.sql(\"\"\"CREATE TABLE partitioned_flights USING parquet PARTITIONED BY (DEST_COUNTRY_NAME)\n",
        "AS SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 5\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04GCUnp1TfSp"
      },
      "source": [
        "# It is possible to create a table from a query as well:\n",
        "spark.sql(\"\"\"CREATE TABLE flights_from_select USING parquet AS SELECT * FROM flights\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBflGpfOTfSp"
      },
      "source": [
        "Show and drop tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqlG46NcTfSp",
        "outputId": "48985b37-ee48-4e70-ec9a-dab3f9bc897b"
      },
      "source": [
        "spark.sql('show tables').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-------------+-----------+\n",
            "|database|    tableName|isTemporary|\n",
            "+--------+-------------+-----------+\n",
            "| default|      flights|      false|\n",
            "| default|  flights_csv|      false|\n",
            "|        |some_sql_view|       true|\n",
            "+--------+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nAf7oXcTfSp",
        "outputId": "8338b219-7b7a-4744-9179-2426eef8ffe1"
      },
      "source": [
        "spark.sql('DROP TABLE tablename')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-S5i4bcTfSp"
      },
      "source": [
        "spark.sql('DROP VIEW IF EXISTS  view_name')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vTNd4NDTfSp"
      },
      "source": [
        "### Inserting into Tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKEDhW5FTfSq"
      },
      "source": [
        "spark.sql(\"\"\"INSERT INTO flights_from_select\n",
        "SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 20\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86ncSg54TfSq"
      },
      "source": [
        "### Describing Table Metadata\n",
        "View comment(table meta data) that created a table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxvywDFnTfSq",
        "outputId": "8e993d1d-3901-41d3-e64e-7822cbbc6c4a"
      },
      "source": [
        "spark.sql(\"\"\"DESCRIBE TABLE flights_csv\"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+---------+--------------------+\n",
            "|           col_name|data_type|             comment|\n",
            "+-------------------+---------+--------------------+\n",
            "|  DEST_COUNTRY_NAME|   string|                null|\n",
            "|ORIGIN_COUNTRY_NAME|   string|remember, the US ...|\n",
            "|              count|   bigint|                null|\n",
            "+-------------------+---------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqYWSHdSTfSq"
      },
      "source": [
        "#see the partitioning\n",
        "spark.sql(\"\"\"SHOW PARTITIONS partitioned_flights\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cjMJTNrTfSq"
      },
      "source": [
        "### Refreshing Table Metadata\n",
        "ensure that you’re reading from the most\n",
        "recent set of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VapBW736TfSq",
        "outputId": "68144a6f-9bac-43d9-a8d7-51bbae363e29"
      },
      "source": [
        "spark.sql(\"\"\"REFRESH table flights_csv\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So_aOwl5TfSq"
      },
      "source": [
        "## View\n",
        "A view specifies a set\n",
        "of transformations on top of an existing table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ed0pSnNTfSq"
      },
      "source": [
        "### Create Views\n",
        "basically just saved query plans for reusing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qT5-jc1TfSq",
        "outputId": "c8b60efa-9ca6-4944-b3e6-f05389f9df81"
      },
      "source": [
        "spark.sql('''\n",
        "create view just_usa_view AS\n",
        "select *\n",
        "from flights\n",
        "where dest_country_name = \"United States\" ''')\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "select * from just_usa_view limit 3\"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|            Ireland|  344|\n",
            "+-----------------+-------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUGCnCQ9TfSr"
      },
      "source": [
        "Like tables, you can create temporary views that are available only during the current session and\n",
        "are not registered to a database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sHjsYq0TfSr",
        "outputId": "55073ee5-11fa-4fc3-e94d-108b4bc101b6"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "CREATE OR REPLACE TEMP VIEW just_usa_view_temp AS \n",
        "SELECT * FROM flights WHERE dest_country_name = 'United States' \n",
        "\"\"\")\n",
        "# OR REPLACE -> overwrite if already exist\n",
        "# TEMP -> exist only in this session"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGGP5JMaTfSr"
      },
      "source": [
        "## Databases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmzeejWlTfSr",
        "outputId": "c3afc602-25ca-4c1d-9e83-dca327b4c862"
      },
      "source": [
        "spark.sql('show databases').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+\n",
            "|databaseName|\n",
            "+------------+\n",
            "|     default|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1OsPQmGTfSr"
      },
      "source": [
        "### Create database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKBzkURETfSr",
        "outputId": "03f9e730-7445-4ae1-9c96-049f85551f7d"
      },
      "source": [
        "spark.sql('create database spark_def')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfy6Q2yHTfSr",
        "outputId": "132d1185-fcdc-4cd9-e935-dc58cb19f59d"
      },
      "source": [
        "# show current data base in use\n",
        "spark.sql('''SELECT current_database()''').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+\n",
            "|current_database()|\n",
            "+------------------+\n",
            "|           default|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4j7agoOTfSr"
      },
      "source": [
        "### Setting and dropping databases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_bbjM-eTfSu",
        "outputId": "93f40d32-554f-4767-d9a7-759ec0d34c94"
      },
      "source": [
        "spark.sql('use spark_def')\n",
        "spark.sql('''SELECT current_database()''').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+\n",
            "|current_database()|\n",
            "+------------------+\n",
            "|         spark_def|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj2hlKFVTfSu",
        "outputId": "09e23163-24f1-41ca-f2fc-e52746715992"
      },
      "source": [
        "# Error table not found because flights is in defaul database\n",
        "try:\n",
        "    spark.sql(\"\"\"select * from flights\"\"\").show()\n",
        "except:\n",
        "    print('tables not found in this database')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tables not found in this database\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCbhnWO3TfSu",
        "outputId": "9836b664-adad-4829-e98e-13ea8e199485"
      },
      "source": [
        "# specify databases name whem cross query database\n",
        "spark.sql(\"\"\"select * from default.flights limit 2\"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-TFXlDeTfSu",
        "outputId": "723b61a5-04c9-4ae9-86ce-16c3c95a1ef2"
      },
      "source": [
        "# drop database/ only when database is empty\n",
        "spark.sql(\"\"\" drop database if exists spark_def\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4hyfxUsTfSu"
      },
      "source": [
        "## Select Statements\n",
        "all options from SQL (ANSI SQL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfQvSos2TfSv"
      },
      "source": [
        "SELECT [ALL|DISTINCT] named_expression[, named_expression, ...]  \n",
        "FROM relation[, relation, ...]  \n",
        "    [lateral_view[, lateral_view, ...]]  \n",
        "    [WHERE boolean_expression]  \n",
        "    [aggregation [HAVING boolean_expression]]  \n",
        "    [ORDER BY sort_expressions]  \n",
        "    [CLUSTER BY expressions]  \n",
        "    [DISTRIBUTE BY expressions]  \n",
        "    [SORT BY sort_expressions]  \n",
        "    [WINDOW named_window[, WINDOW named_window, ...]]  \n",
        "    [LIMIT num_rows]  \n",
        "named_expression:  \n",
        "    : expression [AS alias]  \n",
        "relation:  \n",
        "    | join_relation  \n",
        "    | (table_name|query|relation) [sample] [AS alias]  \n",
        "    : VALUES (expressions)[, (expressions), ...]  \n",
        "        [AS (column_name[, column_name, ...])]  \n",
        "expressions:  \n",
        "    : expression[, expression, ...]  \n",
        "sort_expressions:  \n",
        "    : expression [ASC|DESC][, expression [ASC|DESC], ...]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM01-6e5TfSv"
      },
      "source": [
        "page 198"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SROVQ-76TfSv"
      },
      "source": [
        "## Complex Types in SQL\n",
        "### Structs \n",
        "nested data in spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3B-qMFmTfSv",
        "outputId": "45cc4bd0-926e-4eec-a877-47d6b4835cd0"
      },
      "source": [
        "# wrap a set of columns (or expressions) in parentheses\n",
        "spark.sql(\"\"\"\n",
        "CREATE VIEW IF NOT EXISTS nested_data AS \n",
        "    SELECT (DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME) as country, count \n",
        "    FROM flights\"\"\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT country as DEST_ORIGIN , count\n",
        "FROM nested_data\n",
        "LIMIT 5\"\"\").show(5,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------+-----+\n",
            "|DEST_ORIGIN             |count|\n",
            "+------------------------+-----+\n",
            "|[United States, Romania]|15   |\n",
            "|[United States, Croatia]|1    |\n",
            "|[United States, Ireland]|344  |\n",
            "|[Egypt, United States]  |15   |\n",
            "|[United States, India]  |62   |\n",
            "+------------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-1ocQ7UTfSv"
      },
      "source": [
        "You can even query individual columns within a struct—all you need to do is use dot syntax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfDQTtWhTfSv",
        "outputId": "288eb710-68ae-446b-9780-818544978296"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT country.ORIGIN_COUNTRY_NAME as ORIGIN , count\n",
        "FROM  nested_data\n",
        "\"\"\").show(5,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-----+\n",
            "|ORIGIN       |count|\n",
            "+-------------+-----+\n",
            "|Romania      |15   |\n",
            "|Croatia      |1    |\n",
            "|Ireland      |344  |\n",
            "|United States|15   |\n",
            "|India        |62   |\n",
            "+-------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G5XzOYVTfSv"
      },
      "source": [
        "Or return all sub-column in the nested column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egr_qRG5TfSv",
        "outputId": "0f57f8fd-b635-4218-f3e0-0b2134a656dc"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT country.* \n",
        "FROM  nested_data\n",
        "\"\"\").show(5,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
            "+-----------------+-------------------+\n",
            "|United States    |Romania            |\n",
            "|United States    |Croatia            |\n",
            "|United States    |Ireland            |\n",
            "|Egypt            |United States      |\n",
            "|United States    |India              |\n",
            "+-----------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjAd76lqTfSv"
      },
      "source": [
        "### Lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGOWXPPyTfSv"
      },
      "source": [
        "Use collect_list or collect_set to create aggregate list group by one column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aRRaaJdTfSw",
        "outputId": "d2085b75-88b5-4a04-fd5d-32b7283ee390"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT DEST_COUNTRY_NAME as new_name, collect_list(count) as flight_counts,\n",
        "    collect_set(ORIGIN_COUNTRY_NAME) as origin_set\n",
        "FROM flights \n",
        "GROUP BY DEST_COUNTRY_NAME\n",
        "ORDER BY DEST_COUNTRY_NAME DESC\n",
        "\"\"\").show(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+--------------------+--------------------+\n",
            "|     new_name|       flight_counts|          origin_set|\n",
            "+-------------+--------------------+--------------------+\n",
            "|       Zambia|                 [1]|     [United States]|\n",
            "|    Venezuela|               [290]|     [United States]|\n",
            "|      Uruguay|                [43]|     [United States]|\n",
            "|United States|[145, 225, 15, 1,...|[Italy, Martiniqu...|\n",
            "+-------------+--------------------+--------------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AZ0NK2KTfSw"
      },
      "source": [
        "Then convert array back into rows using *explode* function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCGoqCp8TfSw"
      },
      "source": [
        "SELECT explode(collected_counts), DEST_COUNTRY_NAME FROM flights_agg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ZtsbxETfSw"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZubEve3TfSw",
        "outputId": "d9655966-b0a4-4a61-a38a-909007f4d7df"
      },
      "source": [
        "# list of all user-define SQL functions\n",
        "spark.sql(''' SHOW  USER FUNCTIONS ''').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|function|\n",
            "+--------+\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7bxrR8bTfSw"
      },
      "source": [
        "# Part 12 Resilient Distributed Datasets (RDDs)\n",
        "\n",
        "Working with low-level API(RDDs) when want more control ,such as implement some custom partitioner, physical distribution of data (custom partitioning of data)., or update and track the value\n",
        "of a variable over the course of a data pipeline’s execution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2DavWnZTfSw",
        "outputId": "6ef2f27a-60f0-4357-d3df-360afcacabd3"
      },
      "source": [
        "# access lowe-level API with sparkContext\n",
        "spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://PK:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>SparkApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local appName=SparkApp>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKtSqj5PTfSw"
      },
      "source": [
        "## RDD\n",
        "\n",
        "RDD is an immutable, partitioned collection of records that can be operated\n",
        "on in parallel. Unlike DataFrames though, where each record is a structured row containing\n",
        "fields with a known schema, in RDDs the records are just Java, Scala, or Python objects of the\n",
        "programmer’s choosing\n",
        "\n",
        "So,You can store anything you want in these objects, in any format you want. But come with a lot of manual tasks.  \n",
        "However, it is\n",
        "trivial to convert back and forth between RDDs and Datasets to take advantage of both API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wTvjCVETfSx"
      },
      "source": [
        "## Create RDDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-Vc7K48TfSx"
      },
      "source": [
        "### Convert DataFrames to RDDs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSjocvZtTfSx",
        "outputId": "5724babb-4530-4db6-d2d7-2d0178f6ac73"
      },
      "source": [
        "spark.range(10).rdd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MapPartitionsRDD[40] at javaToPython at <unknown>:0"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQB_b5tbTfSx",
        "outputId": "53dc0171-70d3-431b-9680-935e7de7cb74"
      },
      "source": [
        "# need to convert Row object to RDD of type Row\n",
        "spark.range(10).toDF('id').rdd.map(lambda row: row[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PythonRDD[34] at RDD at PythonRDD.scala:53"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbDZvUNrTfSx"
      },
      "source": [
        "### From a local Collection\n",
        "use paralellize on SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCpA0H24TfSx",
        "outputId": "2ada3875-8c87-43f9-fa91-43bd38ed33a1"
      },
      "source": [
        "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
        "words = spark.sparkContext.parallelize(myCollection, numSlices=2)\n",
        "# can set name of RDD\n",
        "words.setName('myWords')\n",
        "\n",
        "words.name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'myWords'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3klFRRH6TfSx"
      },
      "source": [
        "### From Data Sources\n",
        "90% of the time Data Source from DataFrame API will better then than RDD. But you can do this with"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SjUyn1ITfSx"
      },
      "source": [
        "spark.sparkContext.textFile('some.txt') # each record in the RDD represents a line in that text file or files\n",
        "spark.sparkContext.wholeTextFiles(\"withTextFiles\") #each text file should become a single record"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMjBcm8fTfSx"
      },
      "source": [
        "## Manipulating RDDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKPxeEewTfSy"
      },
      "source": [
        "## Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klb1vPZGTfSy"
      },
      "source": [
        "### Distinct\n",
        "Remove duplicates from RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9365tE5NTfSy",
        "outputId": "9c531ca9-e341-49ea-8a93-c4f3e00e9559"
      },
      "source": [
        "words.distinct().count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC5moF-1TfSy"
      },
      "source": [
        "### filter\n",
        "Use function to return Boolean type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YuSdXdATfSy"
      },
      "source": [
        "def startsWithS(individual):\n",
        "    return individual.startsWithS('S')\n",
        "\n",
        "# words.filter(lambda x: startsWithS(x)).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha8Tp_KQTfSy"
      },
      "source": [
        "Page 221"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpFqwjyoTfSy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6GMiVAaTfSz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QFOGwyrTfSz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57VruRWaTfSz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA5lG2s0TfSz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNREa-KpTfSz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze7aPGaHTfSz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pql5PD26TfSz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM5Jgol3TfSz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxUFKKUPTfSz"
      },
      "source": [
        "# Part 15 How Spark runs on Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVAvGtaRTfSz"
      },
      "source": [
        "## Execution Modes\n",
        "Choosing the execution\n",
        "mode when running your applications. It gives you the power to determine where the aforementioned resources are\n",
        "physically located when you go to run your application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K161cFTyTfSz"
      },
      "source": [
        "### Cluster mode (All job on cluster)\n",
        "User submit script to a cluster manager. the cluster manager is responsible for maintaining all Spark\n",
        "Application–related processes(launches the driver process on a worker node)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgH_HGeUTfS3"
      },
      "source": [
        "### Client mode (Client mange driver, run job on cluster)\n",
        "the Spark driver remains on the client\n",
        "machine that submitted the application. Client machine is responsible for\n",
        "maintaining the Spark driver process, and the cluster manager maintains the executor processses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU47T2JuTfS3"
      },
      "source": [
        "### Local mode (All job on single machine)\n",
        "for test and experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHFkPbexTfS3"
      },
      "source": [
        "## The Life Cycle of a Spark Application (Outside Spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y2D0EdNTfS3"
      },
      "source": [
        "### 1. Client Request\n",
        "\n",
        "request to a driver node asking for resources(spark-submit) for driver process then the application is running at the cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lV20JrPTfS4"
      },
      "source": [
        "# in terminal\n",
        "./bin/spark-submit \\\n",
        "--class <main-class> \\\n",
        "--master <master-url> \\\n",
        "--deploy-mode cluster \\\n",
        "--conf <key>=<value> \\\n",
        "... # other options\n",
        "<application-jar> \\\n",
        "[application-arguments]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0iqQS7ATfS4"
      },
      "source": [
        "## The Life Cycle of a Spark Application (Inside Spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGJNAE3OTfS4"
      },
      "source": [
        "### 1. SparkSession\n",
        "Create SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcA2dCQSTfS4"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Word Count\")\\\n",
        ".config(\"spark.some.config.option\", \"some-value\")\\\n",
        ".getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX4nvlXHTfS4"
      },
      "source": [
        "### A Spark Job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BOCJn35TfS4"
      },
      "source": [
        "In general, there should be one Spark job for one action. Actions always return results. Each job\n",
        "breaks down into a series of stages and stages break into tasks(partitions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAyo8yfRTfS4"
      },
      "source": [
        "### Stages\n",
        "roups of tasks that can be executed together to compute the same\n",
        "operation on multiple machines.  \n",
        "but the engine starts\n",
        "new stages after operations called shuffles. A shuffle represents a physical repartitioning of the\n",
        "data—for example, sorting a DataFrame, or grouping data that was loaded from a file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_S1Whs2TfS5"
      },
      "source": [
        "### Tasks\n",
        "Each task corresponds to a combination of blocks of data and a\n",
        "set of transformations that will run on a single executor. If there is one big partition in our\n",
        "dataset, we will have one task. If there are 1,000 little partitions, we will have 1,000 tasks that\n",
        "can be executed in parallel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntHPqwIFTfS5"
      },
      "source": [
        "### Pipelining\n",
        "In-memory computation -> Spark performs as many steps at once before writing data to memory or disk.  \n",
        "Pipelining collapse all operation in the same nodes into single stage of tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxBa9P0JTfS5"
      },
      "source": [
        "### Shuffle Persistence\n",
        "When Spark run an\n",
        "operation that move data across nodes, such as a reduce-by-key operation it performs a cross-network shuffle.   \n",
        "Spark always executes\n",
        "shuffles by first having the “source” tasks (those sending data) write shuffle files to their local\n",
        "disks during their execution stage.   \n",
        "Then, the stage that does the grouping and reduction launches\n",
        "and runs tasks that fetch their corresponding records from each shuffle file and performs that\n",
        "computation.  \n",
        "Saving the shuffle\n",
        "files to disk lets Spark run this stage later in time than the source stage and relaunch again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbCtYmDfTfS7"
      },
      "source": [
        "# Part 16 Developing Spark Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDKQ9hONTfS8"
      },
      "source": [
        "Specify a certain script as an executable script that builds the SparkSession. This is the\n",
        "one that we will pass as the main argument to spark-submit and simply execute the script\n",
        "against the cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9zTyOuUTfS8"
      },
      "source": [
        "# in Python\n",
        "from __future__ import print_function\n",
        "if __name__ == '__main__':\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder \\\n",
        "        .master(\"local\") \\\n",
        "        .appName(\"Word Count\") \\\n",
        "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "        .getOrCreate()\n",
        "    \n",
        "    print(spark.range(5000).where(\"id > 500\").selectExpr(\"sum(id)\").collect())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VLkYMANTfS8"
      },
      "source": [
        "### Running the application\n",
        "\n",
        "call spark-submit with that\n",
        "information:\n",
        "\n",
        "$SPARK_HOME/bin/spark-submit --master local pyspark_template/main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N9-CJ5FTfS8"
      },
      "source": [
        "### The Development Process\n",
        "First, you might maintain a scratch space, such as an interactive\n",
        "notebook or some equivalent thereof, and then as you build key components and algorithms, you\n",
        "move them to a more permanent location like a library or package\n",
        "\n",
        "When running on your local machine, the spark-shell and its various language-specific\n",
        "implementations are probably the best way to develop applications. For the most part, the shell is\n",
        "for interactive applications, whereas spark-submit is for production applications on your Spark\n",
        "cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSbOT6GjTfS8"
      },
      "source": [
        "# Part 17 Deploying Spark\n",
        "\n",
        "Spark has three officially supported cluster managers:  \n",
        "Standalone mode  \n",
        "Hadoop YARN  \n",
        "Apache Mesos  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NXotLXaTfS9"
      },
      "source": [
        "**On-Premise Spark** (Physical Cluster)  \n",
        "\n",
        "An on-premises cluster gives you full control over the hardware used,\n",
        "meaning you can optimize performance for your specific workload. \n",
        "\n",
        "However, it also introduces\n",
        "some challenges, 1. your cluster is fixed in size, whereas the resource demands of data\n",
        "analytics workloads are often elastic. If you make your cluster too small, it will be hard to launch\n",
        "the occasional very large analytics query or training job for a new machine learning model,\n",
        "whereas if you make it large, you will have resources sitting idle.  \n",
        "2.need setting up georeplication and disaster recovery if\n",
        "required.\n",
        "**Spark in the Cloud**\n",
        "\n",
        "Benefit are First, resources can be launched and shut down elastically,\n",
        "so you can run that occasional “monster” job that takes hundreds of machines for a few hours.\n",
        "\n",
        "Setup global storage systems that are decoupled\n",
        "from a specific cluster, such as Amazon S3, Azure Blob Storage, or Google Cloud Storage and\n",
        "spin up machines dynamically for each Spark workload.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzDDbL-cTfS9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGQjSvpPTfS9"
      },
      "source": [
        "# Part 24 Machine Learning Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEoPr7PSTfS9"
      },
      "source": [
        "## Supervised Learning\n",
        "using historical data that already has labels , train a model\n",
        "to predict the values of those labels based on various features of the data points. \n",
        "\n",
        "This\n",
        "training process usually proceeds through an iterative optimization algorithm such as gradient\n",
        "descent. The training algorithm starts with a basic model and gradually improves it by adjusting\n",
        "various internal parameters (coefficients) during each training iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKbBFao3TfS9"
      },
      "source": [
        "### Recommendation\n",
        "recommendations on what a user may like by\n",
        "drawing similarities between the users or items. By looking at these similarities, the algorithm\n",
        "makes recommendations to users based on what similar users liked, or what other products\n",
        "resemble the ones the user already purchased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQB33mk6TfS9"
      },
      "source": [
        "## Unsupervised learning \n",
        "trying to find patterns or discover the underlying structure in\n",
        "a given set of data. This differs from supervised learning because there is no dependent variable\n",
        "(label) to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwsAVktLTfS9"
      },
      "source": [
        "## MLlib\n",
        "Why use MLlib?  \n",
        "When data is or processing time is too big for a single machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC2nJ5jMTfS9",
        "outputId": "19697c05-8209-43cf-e7da-224df9a2e459"
      },
      "source": [
        "df = spark.read.json('data/simple-ml')\n",
        "df.show(3) # label is good or bad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+----+------+------------------+\n",
            "|color| lab|value1|            value2|\n",
            "+-----+----+------+------------------+\n",
            "|green|good|     1|14.386294994851129|\n",
            "| blue| bad|     8|14.386294994851129|\n",
            "| blue| bad|    12|14.386294994851129|\n",
            "+-----+----+------+------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqTw2JkBTfS9"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "### Transformers \n",
        "When we use MLlib, all inputs to machine learning algorithms  in Spark must consist of type Double (for labels) and\n",
        "Vector[Double] (for features).\n",
        "\n",
        "Using RFormula to transform data (double into vector of double for features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFQ2B56ZTfS9"
      },
      "source": [
        "from pyspark.ml.feature import RFormula\n",
        "supervised = RFormula(formula= 'lab ~ . ') # syntax like in R ,   '.' mean every thing\n",
        "# RFormula auto encode categorical variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehL97pq8TfS-",
        "outputId": "c508d71e-7562-4431-ed8f-fc00ebf90f63"
      },
      "source": [
        "fittedRF = supervised.fit(df)\n",
        "prepareDF = fittedRF.transform(df)\n",
        "prepareDF.show(3,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+----+------+------------------+---------------------------------+-----+\n",
            "|color|lab |value1|value2            |features                         |label|\n",
            "+-----+----+------+------------------+---------------------------------+-----+\n",
            "|green|good|1     |14.386294994851129|[0.0,1.0,1.0,14.386294994851129] |1.0  |\n",
            "|blue |bad |8     |14.386294994851129|[0.0,0.0,8.0,14.386294994851129] |0.0  |\n",
            "|blue |bad |12    |14.386294994851129|[0.0,0.0,12.0,14.386294994851129]|0.0  |\n",
            "+-----+----+------+------------------+---------------------------------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sK7WgHGTfS-"
      },
      "source": [
        "Features vector: [? , color, value1, value 2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aK30pwETfS-",
        "outputId": "66172873-06a9-44b2-d3a3-3b2234ce79a5"
      },
      "source": [
        "from pyspark.ml.feature import RFormula\n",
        "supervised = RFormula(formula= 'lab ~ . + color:value1 + color:value2') #  : is interaction\n",
        "fittedRF = supervised.fit(df)\n",
        "prepareDF = fittedRF.transform(df)\n",
        "prepareDF.show(3,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+----+------+------------------+--------------------------------------------------------------------+-----+\n",
            "|color|lab |value1|value2            |features                                                            |label|\n",
            "+-----+----+------+------------------+--------------------------------------------------------------------+-----+\n",
            "|green|good|1     |14.386294994851129|(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])|1.0  |\n",
            "|blue |bad |8     |14.386294994851129|(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])      |0.0  |\n",
            "|blue |bad |12    |14.386294994851129|(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])    |0.0  |\n",
            "+-----+----+------+------------------+--------------------------------------------------------------------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i04DkNUETfS-"
      },
      "source": [
        "Create train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJAtXFX2TfS-"
      },
      "source": [
        "train, test = prepareDF.randomSplit([0.8,0.2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mjQUoW4TfS-"
      },
      "source": [
        "### Estimators\n",
        "using logisticRegression to fit model  \n",
        "set label and feature column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6pLprEeTfS-",
        "outputId": "d91dc652-88e1-41e5-875e-7d905a536344"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\") # specify feature and label column names\n",
        "print(lr.explainParams()) # inspect the parameters"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
            "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
            "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
            "featuresCol: features column name. (default: features, current: features)\n",
            "fitIntercept: whether to fit an intercept term. (default: True)\n",
            "labelCol: label column name. (default: label, current: label)\n",
            "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
            "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
            "maxIter: max number of iterations (>= 0). (default: 100)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
            "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
            "regParam: regularization parameter (>= 0). (default: 0.0)\n",
            "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
            "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
            "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
            "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
            "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
            "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
            "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9VKcwz-TfS-",
        "outputId": "612fb183-3313-4223-c325-0311446146fa"
      },
      "source": [
        "# fit the model\n",
        "fittedLR = lr.fit(train)\n",
        "\n",
        "# make prediction with transfrom\n",
        "fittedLR.transform(train).select(\"label\", \"prediction\").show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+----------+\n",
            "|label|prediction|\n",
            "+-----+----------+\n",
            "|  0.0|       0.0|\n",
            "|  0.0|       0.0|\n",
            "|  0.0|       0.0|\n",
            "+-----+----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpnOhd2QTfS-"
      },
      "source": [
        "## Pipeline Work flow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTTa0PzCTfS-"
      },
      "source": [
        "Step 1 \n",
        "Create train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwoUd7gbTfS_"
      },
      "source": [
        "train, test = df.randomSplit([0.8,0.2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AOdxWD6TfS_"
      },
      "source": [
        "Create each stage in pipeline (RFormula, LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkQiWVmjTfS_"
      },
      "source": [
        "rForm = RFormula()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7M5UsrnTfS_"
      },
      "source": [
        "lr = LogisticRegression().setLabelCol('label').setFeaturesCol('features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNhadYE2TfS_"
      },
      "source": [
        "# set up pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "stages = [rForm,lr]\n",
        "pipeline = Pipeline().setStages(stages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmSwHenlTfS_"
      },
      "source": [
        "### Training and Evaluation\n",
        "\n",
        "train several variations of the model by\n",
        "specifying different combinations of hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx3ac-KJTfS_"
      },
      "source": [
        "# GridBuilder (GridSearch)\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "params = ParamGridBuilder()\\\n",
        "    .addGrid(rForm.formula, [\n",
        "    \"lab ~ . + color:value1\",\n",
        "    \"lab ~ . + color:value1 + color:value2\"])\\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
        "    .addGrid(lr.regParam, [0.1, 2.0])\\\n",
        "    .build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6D6jxxGTfS_"
      },
      "source": [
        "# evaluators\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator()\\\n",
        "    .setMetricName(\"areaUnderROC\")\\\n",
        "    .setRawPredictionCol(\"prediction\")\\\n",
        "    .setLabelCol(\"label\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5v8t7wGTfS_"
      },
      "source": [
        "**Tuning hyper-parameter**  \n",
        "- TrainValidationSplit :arbitrary random split of our data into two different groups\n",
        "- CrossValidator :performs K-fold cross-validation by splitting the dataset into k nonoverlapping,randomly partitioned folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk_SCU1fTfS_"
      },
      "source": [
        "from pyspark.ml.tuning import TrainValidationSplit\n",
        "tvs = TrainValidationSplit()\\\n",
        "    .setTrainRatio(0.75)\\\n",
        "    .setEstimatorParamMaps(params)\\\n",
        "    .setEstimator(pipeline)\\\n",
        "    .setEvaluator(evaluator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Xf1FoTTfTA"
      },
      "source": [
        "# pretend to load new data for pipeline to create features\n",
        "df = spark.read.json('data/simple-ml')\n",
        "train,test = df.randomSplit([0.7,0.3])\n",
        "# fit the model to gridsearch, pipeline, evaluator\n",
        "tvsFitted = tvs.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2BMu2K0TfTA",
        "outputId": "c9068294-fcde-4aeb-9ea9-94cdb621018f"
      },
      "source": [
        "# evaluate\n",
        "evaluator.evaluate(tvsFitted.transform(test)) # auc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9130434782608696"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqQD6SHfTfTA"
      },
      "source": [
        "See all metric available for some models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJGUrcoHTfTA"
      },
      "source": [
        "### Persisting and Applying Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNsrHIduTfTA",
        "outputId": "eb90245b-beb2-415d-c209-6885e7d485f1"
      },
      "source": [
        "# persisted to disk to use later\n",
        "tvsFitted.write.save('tmp/modelLocation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'function' object has no attribute 'save'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-71-238fe0ed964a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# persisted to disk to use later\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtvsFitted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/tmp/modelLocation'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'save'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0Z8cnhDTfTA"
      },
      "source": [
        "# load model (need to use same estimator)\n",
        "model = TrainValidationSplit.load('tmp/modelLocation')\n",
        "#apply model\n",
        "model.transform(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMBAsjx4TfTA"
      },
      "source": [
        "## Deployment Patterns\n",
        "\n",
        "Train your machine learning (ML) model offline and then supply it with offline data. In\n",
        "this context, we mean offline data to be data that is stored for analysis, and not data that\n",
        "you need to get an answer from quickly. Spark is well suited to this sort of deployment.\n",
        "\n",
        "Train your model offline and then **put the results into a database** (usually a key-value\n",
        "store). This works well for something like recommendation but poorly for something\n",
        "like classification or regression where you cannot just look up a value for a given user\n",
        "but must calculate one based on the input.\n",
        "\n",
        "Train your ML algorithm offline, **persist the model to disk**, and then use that for serving.\n",
        "This is not a low-latency solution if you use Spark for the serving part, as the overhead\n",
        "of starting up a Spark job can be high, even if you’re not running on a cluster.\n",
        "Additionally this does not parallelize well, so you’ll likely have to **put a load balancer in\n",
        "front of multiple model replicas and build out some REST API integration yourself**.\n",
        "\n",
        "Train your ML algorithm online and use it online. This is possible when used in\n",
        "conjunction with Structured Streaming, but can be complex for some models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijbsZ-Y6TfTA"
      },
      "source": [
        "# Part 25 Preprocess and Feature Engineering\n",
        "page 419"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arpgkb_lTfTB"
      },
      "source": [
        "# Part 26 Classification\n",
        "Multiclass = There is more than 2 class, example is MNIS to classify number from 0 to 9  \n",
        "Multilabel = one instance could be classify in to one or more class, example many people in one single photo  \n",
        "** In order to train a multilabel\n",
        "model, you must train one model per label and combine them manually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ7my1ZVTfTB"
      },
      "source": [
        "# load sample data\n",
        "bInput = spark.read.format(\"parquet\").load(\"data/binary-classification\")\\\n",
        ".selectExpr(\"features\", \"cast(label as double) as label\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LxbcYqHTfTB"
      },
      "source": [
        "## Logistic Regression\n",
        "linear method that\n",
        "combines each of the individual inputs (or features) with specific weights (these weights are\n",
        "generated during the training process) that are then combined to get a probability of belonging to\n",
        "a particular class.  \n",
        "\n",
        "These weights are helpful because they are good representations of feature\n",
        "importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYFl6vveTfTB"
      },
      "source": [
        "### Model Hyperparameters\n",
        "- family :multinomial (two or more distinct labels; multiclass classification) or binary (only two distinct labels; binary classification).\n",
        "- elasticNetParam: [0 to 1] L1 and L2 mix\n",
        "- regParam: [>0] how much weight to give to the regularization term in the objective function.\n",
        "- standardization: standardize before put into fucntion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWGVtbFNTfTB"
      },
      "source": [
        "### Training Parameters\n",
        "- maxIter: Total number of iterations over the data before stopping\n",
        "- tol: threshold by which changes in parameters show that we optimized our weights enough, and can stop iterating\n",
        "- weightCol: weigh certain rows more than others. This can be a useful tool if you have some other measure of how important a particular training example is and have a weight associated with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "757xLvaKTfTB"
      },
      "source": [
        "### Prediction Parameters\n",
        "- threshold: Double[0,1] ,probability threshold for when a given class should be predicted. balance between false positives and false negatives\n",
        "- thresholds: array of threshold values for each class when using multiclass classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ0EKvk3TfTB",
        "outputId": "c7c3abed-c3f7-4adf-97ce-0bb15d26e9b8"
      },
      "source": [
        "bInput.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+-----+\n",
            "|      features|label|\n",
            "+--------------+-----+\n",
            "|[3.0,10.1,3.0]|  1.0|\n",
            "|[1.0,0.1,-1.0]|  0.0|\n",
            "|[1.0,0.1,-1.0]|  0.0|\n",
            "+--------------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAKR6x8DTfTB"
      },
      "source": [
        "#Example\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lrModel = lr.fit(bInput)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jaIUlHgTfTB",
        "outputId": "26375077-cd25-4142-eac9-80f329898992"
      },
      "source": [
        "# see coefficients\n",
        "print (lrModel.coefficients)\n",
        "print (lrModel.intercept)\n",
        "# For a multinomial model (the current one is binary), lrModel.coefficientMatrix and lrModel.interceptVector can be used to get the coefficients and intercept"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6.848741326854929,0.3535658901019745,14.814900276915923]\n",
            "-10.22569586448109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMvnZ6jlTfTB",
        "outputId": "4b49c39d-d828-4dec-aebc-f12f88843ece"
      },
      "source": [
        "#sumamry\n",
        "summary = lrModel.summary\n",
        "print (summary.areaUnderROC)\n",
        "summary.roc.show()\n",
        "summary.pr.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n",
            "+---+------------------+\n",
            "|FPR|               TPR|\n",
            "+---+------------------+\n",
            "|0.0|               0.0|\n",
            "|0.0|0.3333333333333333|\n",
            "|0.0|               1.0|\n",
            "|1.0|               1.0|\n",
            "|1.0|               1.0|\n",
            "+---+------------------+\n",
            "\n",
            "+------------------+---------+\n",
            "|            recall|precision|\n",
            "+------------------+---------+\n",
            "|               0.0|      1.0|\n",
            "|0.3333333333333333|      1.0|\n",
            "|               1.0|      1.0|\n",
            "|               1.0|      0.6|\n",
            "+------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7UrblDITfTB",
        "outputId": "d011c9d3-77ee-415f-9e55-5fd50d42d71d"
      },
      "source": [
        "summary.objectiveHistory\n",
        "# specify how, over each training iteration, we are performing with respect to our objective function"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.6730116670092563,\n",
              " 0.5042829330409727,\n",
              " 0.36356862066874396,\n",
              " 0.1252407018038337,\n",
              " 0.08532556611276214,\n",
              " 0.035504876415730455,\n",
              " 0.018196494508571255,\n",
              " 0.008817369922959136,\n",
              " 0.004413673785392143,\n",
              " 0.002194038351234709,\n",
              " 0.0010965641148080857,\n",
              " 0.000547657551985314,\n",
              " 0.00027376237951490126,\n",
              " 0.0001368465223657475,\n",
              " 6.841809037070595e-05,\n",
              " 3.420707791038497e-05,\n",
              " 1.7103176664232043e-05,\n",
              " 8.551470106426904e-06,\n",
              " 4.275703677941461e-06,\n",
              " 2.1378240117781303e-06,\n",
              " 1.068856405465203e-06,\n",
              " 5.34260020257524e-07,\n",
              " 2.668135105897439e-07,\n",
              " 1.3204627865316843e-07,\n",
              " 6.768401481686428e-08,\n",
              " 3.314547718487037e-08,\n",
              " 1.6151438837494788e-08,\n",
              " 8.309350118269286e-09]"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq6jiCeXTfTC"
      },
      "source": [
        "# Spark Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tpGn2VqTfTC"
      },
      "source": [
        "## DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWuniUQuH39I",
        "outputId": "fc618def-bb47-4382-fde0-7c2e9f98cdd9"
      },
      "source": [
        "%%shell\n",
        "git clone https://github.com/databricks/LearningSparkV2.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LearningSparkV2'...\n",
            "remote: Enumerating objects: 1712, done.\u001b[K\n",
            "remote: Counting objects: 100% (135/135), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 1712 (delta 38), reused 85 (delta 23), pack-reused 1577\u001b[K\n",
            "Receiving objects: 100% (1712/1712), 76.76 MiB | 20.12 MiB/s, done.\n",
            "Resolving deltas: 100% (525/525), done.\n",
            "Checking out files: 100% (768/768), done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 253
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu_vnTHAH9HV",
        "outputId": "53dea57e-2248-4829-c568-f46a93cddd41"
      },
      "source": [
        "%%shell\n",
        "head -3 LearningSparkV2/chapter3/data/sf-fire-calls.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CallNumber,UnitID,IncidentNumber,CallType,CallDate,WatchDate,CallFinalDisposition,AvailableDtTm,Address,City,Zipcode,Battalion,StationArea,Box,OriginalPriority,Priority,FinalPriority,ALSUnit,CallTypeGroup,NumAlarms,UnitType,UnitSequenceInCallDispatch,FirePreventionDistrict,SupervisorDistrict,Neighborhood,Location,RowID,Delay\n",
            "20110016,T13,2003235,Structure Fire,01/11/2002,01/10/2002,Other,01/11/2002 01:51:44 AM,2000 Block of CALIFORNIA ST,SF,94109,B04,38,3362,3,3,3,false,\"\",1,TRUCK,2,4,5,Pacific Heights,\"(37.7895840679362, -122.428071912459)\",020110016-T13,2.95\n",
            "20110022,M17,2003241,Medical Incident,01/11/2002,01/10/2002,Other,01/11/2002 03:01:18 AM,0 Block of SILVERVIEW DR,SF,94124,B10,42,6495,3,3,3,true,\"\",1,MEDIC,1,10,10,Bayview Hunters Point,\"(37.7337623673897, -122.396113802632)\",020110022-M17,4.7\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnxH69v3L2TK",
        "outputId": "0200d9bc-f4a5-4279-a9bb-68682af6621b"
      },
      "source": [
        "sf_path = 'LearningSparkV2/chapter3/data/sf-fire-calls.csv'\n",
        "df = spark.read.csv(\n",
        "    sf_path\n",
        "    ,sep = ','\n",
        "    ,inferSchema= True\n",
        "    ,header = True\n",
        ")\n",
        "df.cache()\n",
        "df.show(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
            "|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|             Address|City|Zipcode|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|        Neighborhood|            Location|        RowID|    Delay|\n",
            "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
            "|  20110016|   T13|       2003235|  Structure Fire|01/11/2002|01/10/2002|               Other|01/11/2002 01:51:...|2000 Block of CAL...|  SF|  94109|      B04|         38|3362|               3|       3|            3|  false|         null|        1|   TRUCK|                         2|                     4|                 5|     Pacific Heights|(37.7895840679362...|020110016-T13|     2.95|\n",
            "|  20110022|   M17|       2003241|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 03:01:...|0 Block of SILVER...|  SF|  94124|      B10|         42|6495|               3|       3|            3|   true|         null|        1|   MEDIC|                         1|                    10|                10|Bayview Hunters P...|(37.7337623673897...|020110022-M17|      4.7|\n",
            "|  20110023|   M41|       2003242|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 02:39:...|MARKET ST/MCALLIS...|  SF|  94102|      B03|         01|1455|               3|       3|            3|   true|         null|        1|   MEDIC|                         2|                     3|                 6|          Tenderloin|(37.7811772186856...|020110023-M41|2.4333334|\n",
            "|  20110032|   E11|       2003250|    Vehicle Fire|01/11/2002|01/10/2002|               Other|01/11/2002 04:16:...|APPLETON AV/MISSI...|  SF|  94110|      B06|         32|5626|               3|       3|            3|  false|         null|        1|  ENGINE|                         1|                     6|                 9|      Bernal Heights|(37.7388432849018...|020110032-E11|      1.5|\n",
            "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QguIr7FrMQ_e"
      },
      "source": [
        "• What were all the different types of fire calls in 2018?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqmFDWBRMRKe",
        "outputId": "1ec359d1-a03c-414b-8c78-23a8e5f8f0bc"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CallNumber',\n",
              " 'UnitID',\n",
              " 'IncidentNumber',\n",
              " 'CallType',\n",
              " 'CallDate',\n",
              " 'WatchDate',\n",
              " 'CallFinalDisposition',\n",
              " 'AvailableDtTm',\n",
              " 'Address',\n",
              " 'City',\n",
              " 'Zipcode',\n",
              " 'Battalion',\n",
              " 'StationArea',\n",
              " 'Box',\n",
              " 'OriginalPriority',\n",
              " 'Priority',\n",
              " 'FinalPriority',\n",
              " 'ALSUnit',\n",
              " 'CallTypeGroup',\n",
              " 'NumAlarms',\n",
              " 'UnitType',\n",
              " 'UnitSequenceInCallDispatch',\n",
              " 'FirePreventionDistrict',\n",
              " 'SupervisorDistrict',\n",
              " 'Neighborhood',\n",
              " 'Location',\n",
              " 'RowID',\n",
              " 'Delay']"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo9jqTyZMRS6",
        "outputId": "dee99124-adba-4f26-c346-799a4e2f8a1c"
      },
      "source": [
        "df.select('CallType').distinct().show(3,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------+\n",
            "|CallType                   |\n",
            "+---------------------------+\n",
            "|Elevator / Escalator Rescue|\n",
            "|Marine Fire                |\n",
            "|Aircraft Emergency         |\n",
            "+---------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoZmLCp6MiBB"
      },
      "source": [
        "• What months within the year 2018 saw the highest number of fire calls?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6ut7r_QM0BZ"
      },
      "source": [
        "from pyspark.sql.functions import to_date,to_timestamp, month,year\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_date = (df\n",
        "           .withColumn('IncidentDate' , to_timestamp(col('CallDate'), 'MM/dd/yyyy')).drop('CallDate')\n",
        "            .withColumn(\"OnWatchDate\",   to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\")).drop(\"WatchDate\")\n",
        "            .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"), \"MM/dd/yyyy hh:mm:ss a\")).drop(\"AvailableDtTm\")\n",
        "            .withColumn('IncidentMonth', month('IncidentDate')) # no need col here\n",
        "            .withColumn('IncidentYear', year('IncidentDate'))\n",
        "            )    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91vrNAX6Mn95",
        "outputId": "b8e5c5c0-e7ac-4a20-a343-ebc33fb131dc"
      },
      "source": [
        "df_date.select('IncidentYear').show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|IncidentYear|\n",
            "+------------+\n",
            "|        2002|\n",
            "|        2002|\n",
            "|        2002|\n",
            "+------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daNHGWhsQFwR",
        "outputId": "2bdf792d-971a-46a1-d96e-931fcf076804"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "(df_date.where(col('IncidentYear') == 2018)\n",
        "    .groupBy('IncidentMonth')\n",
        "    .agg(F.count('CallNumber').alias('total'))\n",
        "    .orderBy(F.desc('total'))\n",
        "    .show(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|IncidentMonth|total|\n",
            "+-------------+-----+\n",
            "|           10| 1068|\n",
            "+-------------+-----+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N-VDQHASO8i"
      },
      "source": [
        "Q-3) Find out all response or delayed times greater than 5 mins?\n",
        "\n",
        "Rename the column Delay - > ReponseDelayedinMins  \n",
        "\n",
        "Returns a new DataFrame  \n",
        "\n",
        "Find out all calls where the response time to the fire site was delayed for more than 5 mins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O4xniDsSQBy",
        "outputId": "ba2aebd6-b327-4fd7-a254-1181dced56b9"
      },
      "source": [
        "df_date.where(F.expr(\"Delay > 5\")).show(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+--------------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------------+--------------------------+----------------------+------------------+------------------+--------------------+-------------+-----+-------------------+-------------------+-------------------+-------------+------------+\n",
            "|CallNumber|UnitID|IncidentNumber|            CallType|CallFinalDisposition|             Address|City|Zipcode|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|      UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|      Neighborhood|            Location|        RowID|Delay|       IncidentDate|        OnWatchDate|      AvailableDtTS|IncidentMonth|IncidentYear|\n",
            "+----------+------+--------------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------------+--------------------------+----------------------+------------------+------------------+--------------------+-------------+-----+-------------------+-------------------+-------------------+-------------+------------+\n",
            "|  20110315|   RC2|       2003409|    Medical Incident|               Other|200 Block of LAGU...|  SF|  94116|      B08|         20|8635|               3|       3|            3|   true|         null|        1|RESCUE CAPTAIN|                         2|                     8|                 7|West of Twin Peaks|(37.7501117393668...|020110315-RC2| 5.35|2002-01-11 00:00:00|2002-01-11 00:00:00|2002-01-11 16:34:23|            1|        2002|\n",
            "|  20120147|   M38|       2003642|    Medical Incident|               Other|1300 Block of HYD...|  SF|  94109|      B01|         41|1564|               1|       1|            2|   true|         null|        1|         MEDIC|                         1|                     1|                 3|      Russian Hill|(37.793235074749,...|020120147-M38| 6.25|2002-01-12 00:00:00|2002-01-12 00:00:00|2002-01-12 13:23:04|            1|        2002|\n",
            "|  20130013|   M03|       2003818|    Medical Incident|               Other|800 Block of LARK...|  SF|  94109|      B04|         03|1642|               2|       2|            2|   true|         null|        1|         MEDIC|                         1|                     4|                 6|        Tenderloin|(37.7858627664608...|020130013-M03|  5.2|2002-01-13 00:00:00|2002-01-12 00:00:00|2002-01-13 01:51:15|            1|        2002|\n",
            "|  20140067|   T19|       2004152|Citizen Assist / ...|               Other|2900 Block of 22N...|  SF|  94132|      B08|         19|8732|               3|       3|            3|  false|         null|        1|         TRUCK|                         1|                     8|                 7|   Sunset/Parkside|(37.7335645188552...|020140067-T19|  5.6|2002-01-14 00:00:00|2002-01-14 00:00:00|2002-01-14 08:16:54|            1|        2002|\n",
            "+----------+------+--------------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------------+--------------------------+----------------------+------------------+------------------+--------------------+-------------+-----+-------------------+-------------------+-------------------+-------------+------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01DrsyEdSior"
      },
      "source": [
        "Q-5) What was the sum of all calls, average, min and max of the response times for calls?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mwLuPX8Szd8",
        "outputId": "4fe4f44e-75d6-449f-c125-d1a541ec2b7c"
      },
      "source": [
        "df.select('Delay').describe().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+\n",
            "|summary|             Delay|\n",
            "+-------+------------------+\n",
            "|  count|            175296|\n",
            "|   mean|3.8923641541750342|\n",
            "| stddev| 9.378286170882737|\n",
            "|    min|       0.016666668|\n",
            "|    max|           1844.55|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}